{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections.abc import Callable\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "from tokenizers import Tokenizer, decoders, normalizers\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import NFD, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from torch import Tensor, nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "PARENT_DIR = \".\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # for Apple chips\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"Helsinki-NLP/opus-100\", \"de-en\")\n",
    "dataset = datasets.load_from_disk(f\"{PARENT_DIR}/datasets/iwslt_de_en_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")\n",
    "\n",
    "MAX_NUM_SAMPLES = None\n",
    "if MAX_NUM_SAMPLES is not None:\n",
    "    train_data = train_data.take(MAX_NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craete_tokenizer(max_vocab_size: int = 4000) -> tuple[Tokenizer, WordPieceTrainer]:\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"<UNK>\"))  # noqa: S106\n",
    "    tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase()])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"<BOS> $A <EOS>\",\n",
    "        special_tokens=[\n",
    "            (\"<BOS>\", 1),\n",
    "            (\"<EOS>\", 2),\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.decoder = decoders.WordPiece()\n",
    "    trainer = WordPieceTrainer(vocab_size=max_vocab_size,\n",
    "                               special_tokens=[\"<UNK>\", \"<BOS>\", \"<EOS>\", \"<PAD>\"])\n",
    "\n",
    "    return tokenizer, trainer\n",
    "\n",
    "def batch_iterator(batch_size: int, data: datasets.Dataset, key: str) -> list[str]:\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        data_batch = data[i : i + batch_size][\"translation\"]\n",
    "        data_key = [x[key] for x in data_batch]\n",
    "        yield data_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_lang, src_lang_trainer = craete_tokenizer()\n",
    "tgt_lang, tgt_lang_trainer = craete_tokenizer()\n",
    "\n",
    "src_lang.train_from_iterator(batch_iterator(256, train_data, \"de\"), src_lang_trainer)\n",
    "tgt_lang.train_from_iterator(batch_iterator(256, train_data, \"en\"), tgt_lang_trainer)\n",
    "\n",
    "src_lang.save(f\"{PARENT_DIR}/de_tokenizer\")\n",
    "tgt_lang.save(f\"{PARENT_DIR}/en_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef58aff46ba49999061b397e3e4774d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/206112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eb28a70d4a45a782845a15dd4fdd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5fc2d4f37242cfbee39587dbfbeb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c883b0a479c45f3bfe20c69ba1a55bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/206112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0ba6eff8c24663b0b3ccd8890b976c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b6c23885d34486ae28c7dbf5797761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_example(example: dict[str, str],\n",
    "                     en_tokenizer: Tokenizer,\n",
    "                     de_tokenizer: Tokenizer,\n",
    "                     max_length: int) -> dict[str, list[str | int]]:\n",
    "    en_encoded = en_tokenizer.encode(example[\"translation\"][\"en\"])\n",
    "    de_encoded = de_tokenizer.encode(example[\"translation\"][\"de\"])\n",
    "\n",
    "    return {\"en_tokens\": en_encoded.tokens[:max_length],\n",
    "            \"de_tokens\": de_encoded.tokens[:max_length],\n",
    "            \"en_ids\": en_encoded.ids[:max_length],\n",
    "            \"de_ids\": de_encoded.ids[:max_length]}\n",
    "\n",
    "max_length = 128\n",
    "sos_token = \"<BOS>\"  # noqa: S105\n",
    "eos_token = \"<EOS>\"  # noqa: S105\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_tokenizer\": tgt_lang,\n",
    "    \"de_tokenizer\": src_lang,\n",
    "    \"max_length\": max_length,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "\n",
    "train_data.save_to_disk(f\"{PARENT_DIR}/train_data_{max_length}\")\n",
    "valid_data.save_to_disk(f\"{PARENT_DIR}/valid_data_{max_length}\")\n",
    "test_data.save_to_disk(f\"{PARENT_DIR}/test_data_{max_length}\")\n",
    "\n",
    "train_data = datasets.load_from_disk(f\"{PARENT_DIR}/train_data_{max_length}\")\n",
    "valid_data = datasets.load_from_disk(f\"{PARENT_DIR}/valid_data_{max_length}\")\n",
    "test_data = datasets.load_from_disk(f\"{PARENT_DIR}/test_data_{max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size src: 4000\n",
      "Vocab size tgt: 4000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size src: {src_lang.get_vocab_size()}\")\n",
    "print(f\"Vocab size tgt: {tgt_lang.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich bin wirklich begeistert von dieser konferenz, und ich danke ihnen allen fuÌˆr die vielen netten kommentare zu meiner rede vorgestern abend.\n",
      "i have been blown away by this conference, and i want to thank all of you for the many nice comments about what i had to say the other night.\n"
     ]
    }
   ],
   "source": [
    "print(src_lang.decode(train_data[2][\"de_ids\"]))\n",
    "print(tgt_lang.decode(train_data[2][\"en_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    "    )\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index: int) -> Callable[[dict], dict]:\n",
    "    def collate_fn(batch: dict) -> dict[str, int]:\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        return {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset: datasets.Dataset, batch_size: int, pad_index: int, *,\n",
    "                    shuffle: bool = True, pin_memory: bool = False) -> torch.utils.data.DataLoader:\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, tgt_lang.token_to_id(\"<PAD>\"))\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, tgt_lang.token_to_id(\"<PAD>\"))\n",
    "test_data_loader = get_data_loader(test_data, batch_size, tgt_lang.token_to_id(\"<PAD>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, dropout_ratio: float = 0.1):\n",
    "        if hidden_dim % num_heads != 0:\n",
    "            msg = \"hidden_dim must be divisible by num_heads\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scaling = 1 / (self.head_dim ** .5)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            q: query of size (batch_size, seq_length, hidden_dim)\n",
    "            k: query of size (batch_size, seq_length, hidden_dim)\n",
    "            v: query of size (batch_size, seq_length, hidden_dim)\n",
    "            mask: optional mask of size (batch_size, 1, 1, seq_length)\n",
    "                  or (batch_size, 1, seq_length, seq_length)\n",
    "        Outputs\n",
    "            attention weighted embedding vectors of size (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # all Q, K, V are of shape (batch_size, seq_length, hidden_dim)\n",
    "        Q = self.fc_q(q)\n",
    "        K = self.fc_k(k)\n",
    "        V = self.fc_v(v)\n",
    "\n",
    "        batch_size, seq_length, _ = Q.size()\n",
    "\n",
    "        # all Q, K, V are of shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # energy.shape (batch_size, num_heads, seq_length, seq_length)\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) * self.scaling  # type: Tensor\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -torch.inf)\n",
    "\n",
    "        # attention.shape (batch_size, num_heads, seq_length, seq_length)\n",
    "        attention = energy.softmax(dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # x.shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, num_heads, head_dim)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, hidden_dim)\n",
    "        x = x.reshape(batch_size, seq_length, self.hidden_dim)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, hidden_dim)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, ff_dim: int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.fc_2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        # x.shape (batch_size, seq_length, emb_dim)\n",
    "        x = self.fc_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, ff_dim:int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.ff = PositionWiseFeedForward(hidden_dim=hidden_dim, ff_dim=ff_dim)\n",
    "\n",
    "        self.mha = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                      num_heads=num_heads,\n",
    "                                      dropout_ratio=dropout_ratio)\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            input of size (batch_size, seq_length, hidden_dim)\n",
    "            mask of size (batch_size, 1, 1, seq_length)\n",
    "        Outputs\n",
    "            (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # x1.shape (batch_size, seq_length, hidden_dim)\n",
    "        x1 = self.mha(src, src, src, mask=mask)  # type: Tensor\n",
    "        x1 = self.norm_1(self.dropout(x1) + src)\n",
    "\n",
    "        # x2.shape (batch_size, seq_length, hidden_dim)\n",
    "        x2 = self.ff(x1)\n",
    "        x2 = self.norm_2(x1 + self.dropout(x2))\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove scaling in encoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int,\n",
    "                 num_heads: int, num_layers: int, ff_dim: int, max_seq_length: int,\n",
    "                 device: torch.device, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.scaling = hidden_dim ** (0.5)\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim=hidden_dim,\n",
    "                                                  num_heads=num_heads, ff_dim=ff_dim,\n",
    "                                                  dropout_ratio=dropout_ratio)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            input of shape (batch_size, seq_legth)\n",
    "        Outputs\n",
    "            encoded sequence of shape (batch_size, seq_legth, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = src.size()\n",
    "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "        x = self.token_embedding(src) * self.scaling + self.positional_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int,\n",
    "                 ff_dim:int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_3 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.num_heads = num_heads\n",
    "        self.ff = PositionWiseFeedForward(hidden_dim=hidden_dim, ff_dim=ff_dim)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                                 num_heads=num_heads,\n",
    "                                                 dropout_ratio=dropout_ratio)\n",
    "\n",
    "        self.enc_attention = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                                num_heads=num_heads,\n",
    "                                                dropout_ratio=dropout_ratio)\n",
    "\n",
    "    def forward(self, dec_input: Tensor, enc_outputs: Tensor,\n",
    "                enc_mask: Tensor, dec_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_input of shape (batch_size, seq_length, hidden_dim)\n",
    "            enc_input of shape (batch_size, seq_length, hidden_dim)\n",
    "            enc_mask of shape (batch_size, 1, 1, seq_length)\n",
    "            dec_mask of shape (batch_size, 1, seq_length, seq_length)\n",
    "        Outputs\n",
    "            (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # x1.shape (batch_size, seq_length, hidden_dim)\n",
    "        x1 = self.self_attention(dec_input, dec_input, dec_input, mask=dec_mask)\n",
    "\n",
    "        x1 = self.norm_1(dec_input + self.dropout(x1))\n",
    "\n",
    "        # x2.shape (batch_size, seq_length, hidden_dim)\n",
    "        x2 = self.enc_attention(x1, enc_outputs, enc_outputs, mask=enc_mask)\n",
    "\n",
    "        x2 = self.norm_2(x1 + self.dropout(x2))\n",
    "\n",
    "        x3 = self.ff(x2)\n",
    "\n",
    "        x3 = self.norm_3(x2 + self.dropout(x3))\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do we need scaling in decoder (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int,\n",
    "                 num_heads: int, num_layers: int, ff_dim: int, max_seq_length: int,\n",
    "                 device: torch.device, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.scaling = hidden_dim ** (0.5)\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim=hidden_dim,\n",
    "                                                  num_heads=num_heads, ff_dim=ff_dim,\n",
    "                                                  dropout_ratio=dropout_ratio)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, dec_input: Tensor, enc_outputs: Tensor,\n",
    "                enc_mask: Tensor, dec_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_inputs of shape (batch_size, seq_legth)\n",
    "            enc_outputs of shape (batch_size, seq_legth, hidden_dim)\n",
    "            dec_mask of shape (batch_size, 1, seq_length, seq_length)\n",
    "            enc_mask of shape (batch_size, 1, 1, seq_length)\n",
    "        Outputs\n",
    "            log-probabilities of shape (batch_size, seq_legth, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = dec_input.size()\n",
    "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "        x = self.token_embedding(dec_input) * self.scaling + self.positional_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "            x = layer(x, enc_outputs, enc_mask, dec_mask)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, vocab_size)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 hidden_dim: int, ff_dim: int, num_heads: int, num_layers: int,\n",
    "                 enc_pad_token: int, dec_pad_token: int,\n",
    "                 max_seq_length: int, dropout_ratio: float, device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(vocab_size=src_vocab_size,\n",
    "                               hidden_dim=hidden_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               num_layers=num_layers,\n",
    "                               ff_dim=ff_dim,\n",
    "                               max_seq_length=max_seq_length,\n",
    "                               device=device,\n",
    "                               dropout_ratio=dropout_ratio)\n",
    "        \n",
    "        \n",
    "        self.decoder = Decoder(vocab_size=tgt_vocab_size,\n",
    "                                hidden_dim=hidden_dim,\n",
    "                                num_heads=num_heads,\n",
    "                                num_layers=num_layers,\n",
    "                                ff_dim=ff_dim,\n",
    "                                max_seq_length=max_seq_length,\n",
    "                                device=device,\n",
    "                                dropout_ratio=dropout_ratio)\n",
    "        \n",
    "        self.enc_pad_token = enc_pad_token\n",
    "        self.dec_pad_token = dec_pad_token\n",
    "        self.device = device\n",
    "\n",
    "    def _create_enc_mask(self, enc_inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            enc_inputs of shape (batch_size, seq_length)\n",
    "        Outputs\n",
    "            mask with 0s for PAD tokens of shape (batch_size, 1, 1, seq_length)\n",
    "        \"\"\"\n",
    "        return (enc_inputs != self.enc_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def _create_dec_mask(self, dec_inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_inputs of shape (batch_size, seq_length)\n",
    "        Outputs\n",
    "            masks PAD tokens and future tokens; shape (batch_size, 1, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "        # mask_1 shape (batch_size, 1, 1, seq_length)\n",
    "        mask_1 = (dec_inputs != self.dec_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        batch_size, seq_length = dec_inputs.size()\n",
    "\n",
    "        # mask_2 shape (seq_length, seq_length)\n",
    "        mask_2 = torch.tril(torch.ones((seq_length, seq_length), device= self.device)).bool()\n",
    "\n",
    "        # mask_2 shape (batch_size, 1, seq_length, seq_length)\n",
    "        mask_2 = mask_2.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "        mask = mask_1 & mask_2\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            src of shape (batch_size, src_seq_length)\n",
    "            tgt of shape (batch_size, tgt_seq_length)\n",
    "        Outputs\n",
    "            decoded sequence of shape (batch_size, seq_length, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        enc_mask = self._create_enc_mask(src)\n",
    "        dec_mask = self._create_dec_mask(tgt)\n",
    "\n",
    "        enc_outputs = self.encoder(src, enc_mask)\n",
    "        dec_outputs = self.decoder(tgt, enc_outputs, enc_mask, dec_mask)\n",
    "\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num parameters: 6,827,936\n"
     ]
    }
   ],
   "source": [
    "def init_weights(model: nn.Module) -> None:\n",
    "    if hasattr(model, \"weight\") and model.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(model.weight.data)\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "loss_function = nn.NLLLoss(ignore_index=tgt_lang.token_to_id(\"<PAD>\"))\n",
    "\n",
    "dropout_ratio = 0.3\n",
    "hidden_dim = 256\n",
    "ff_dim = 1024\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "\n",
    "translator = TranslatorModel(src_vocab_size=src_lang.get_vocab_size(),\n",
    "                             tgt_vocab_size=tgt_lang.get_vocab_size(),\n",
    "                             hidden_dim=hidden_dim,\n",
    "                             ff_dim=ff_dim,\n",
    "                             num_heads=num_heads,\n",
    "                             num_layers=num_layers,\n",
    "                             enc_pad_token=src_lang.token_to_id(\"<PAD>\"),\n",
    "                             dec_pad_token=tgt_lang.token_to_id(\"<PAD>\"),\n",
    "                             max_seq_length=max_length,\n",
    "                             dropout_ratio=dropout_ratio,\n",
    "                             device=device).to(device)\n",
    "\n",
    "translator.apply(init_weights)\n",
    "print(f\"Model num parameters: {count_parameters(translator):,}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(translator.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=10e-9)\n",
    "\n",
    "# def schedule_func(step: int):\n",
    "#     warmup = 200\n",
    "#     step += 1\n",
    "#     return min(step ** -0.5, step * warmup ** -1.5)\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, schedule_func)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 27.31 MB\n"
     ]
    }
   ],
   "source": [
    "size_model = 0\n",
    "for param in translator.parameters():\n",
    "    if param.data.is_floating_point():\n",
    "        size_model += param.numel() * torch.finfo(param.data.dtype).bits\n",
    "    else:\n",
    "        size_model += param.numel() * torch.iinfo(param.data.dtype).bits\n",
    "print(f\"Model size: {size_model / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(model: nn.Module,\n",
    "              loss_function: nn.NLLLoss,\n",
    "              batch: dict[str, Tensor],\n",
    "              device: torch.device) -> float:\n",
    "    # src.shape (batch_size, seq_length)\n",
    "    src = batch[\"de_ids\"].to(device).transpose(1, 0)  # type: Tensor\n",
    "    tgt = batch[\"en_ids\"].to(device).transpose(1, 0)  # type: Tensor\n",
    "\n",
    "    log_probs = model(src, tgt[:, :-1])  # type: Tensor\n",
    "    log_probs = log_probs.reshape(-1, log_probs.size(-1))\n",
    "\n",
    "    loss = loss_function(log_probs, tgt[:, 1:].reshape(-1).long())  # type: Tensor\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    loss_function: nn.NLLLoss,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    device: torch.device,\n",
    "                    scheduler: torch.optim.lr_scheduler.LambdaLR = None) -> float:\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    progress_bar = tqdm(data_loader)\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = run_batch(model=model, loss_function=loss_function, batch=batch, device=device)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(translator.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            running_loss = sum(losses) / len(losses)\n",
    "            progress_bar.set_description(f\"Running loss after {i} batches: {running_loss:.2f}\")\n",
    "\n",
    "    # return sum(losses) / len(losses)\n",
    "    return losses\n",
    "\n",
    "def translate_from_tensor(model: nn.Module, src: Tensor,\n",
    "                          tgt_lang: Tokenizer, tgt: Tensor) -> str:\n",
    "    # outputs.shape (batch_size, tgt_seq_length, tgt_vocab_size)\n",
    "    log_probs = model(src, tgt=tgt)  # type: Tensor\n",
    "\n",
    "    # pred_top2.shape (batch_size, tgt_seq_length, 2)\n",
    "    _, pred_top2 = log_probs.topk(2, dim=-1)\n",
    "\n",
    "    # pred_top2.shape (tgt_seq_length, 2)\n",
    "    pred_top2 = pred_top2.squeeze(0)  # because batch_size=1 here\n",
    "\n",
    "    # unpack first 2 top predictions\n",
    "    first_pred, second_pred = pred_top2[:, 0].unsqueeze(1), pred_top2[:, 1].unsqueeze(1)\n",
    "\n",
    "    # in case first top prediction is UNK use second top prediction\n",
    "    unk_idx = tgt_lang.token_to_id(\"<UNK>\")\n",
    "    indices = torch.where(first_pred == unk_idx, second_pred, first_pred)\n",
    "\n",
    "    indices = indices.squeeze().tolist()\n",
    "\n",
    "    sentence = tgt_lang.decode(indices, skip_special_tokens=False)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def translate(model: TranslatorModel, src: Tensor | str, src_lang: Tokenizer,\n",
    "              tgt_lang: Tokenizer, max_tgt_length: int, device: torch.device,\n",
    "              clean: bool = False) -> str:\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(src, str):\n",
    "        nlp = spacy.load(\"de_core_news_sm\")  # TODO: don't like this workaround\n",
    "        tokens = [token.text.lower() for token in nlp(src)]\n",
    "        tokens = [\"<BOS>\", *tokens, \"<EOS>\"]\n",
    "        src_idxs = np.array([src_lang.token_to_id(word) for word in tokens])\n",
    "        src = torch.from_numpy(src_idxs).reshape(1, -1).to(device)  # (batch_size, src_seq_length)\n",
    "\n",
    "    enc_mask = model._create_enc_mask(src)  # noqa: SLF001\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(src, enc_mask)\n",
    "\n",
    "    tgt_indices = [tgt_lang.token_to_id(\"<BOS>\")]\n",
    "\n",
    "    for _ in range(max_tgt_length):\n",
    "        tgt = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "        dec_mask = model._create_dec_mask(tgt)  # noqa: SLF001\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_probs = model.decoder(tgt, encoder_outputs, enc_mask, dec_mask)  # type: Tensor\n",
    "\n",
    "        # pred_top2.shape (batch_size, tgt_seq_length, 2)\n",
    "        _, pred_top2 = log_probs.topk(2, dim=-1)\n",
    "\n",
    "        # pred_top2.shape (tgt_seq_length, 2)\n",
    "        pred_top2 = pred_top2.squeeze(0)  # because batch_size=1 here\n",
    "\n",
    "        # unpack first 2 top predictions\n",
    "        first_pred, second_pred = pred_top2[-1, 0].item(), pred_top2[-1, 1].item()\n",
    "\n",
    "        # in case first top prediction is UNK use second top prediction\n",
    "        unk_idx = tgt_lang.token_to_id(\"<UNK>\")\n",
    "        pred_token = second_pred if first_pred == unk_idx else first_pred\n",
    "\n",
    "        tgt_indices.append(pred_token)\n",
    "\n",
    "        if pred_token == tgt_lang.token_to_id(\"<EOS>\"):\n",
    "            break\n",
    "\n",
    "    sentence = tgt_lang.decode(tgt_indices, skip_special_tokens=clean)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def print_sentences(data: datasets.Dataset, idx: int, model: nn.Module,\n",
    "                    src_lang: Tokenizer, tgt_lang: Tokenizer, device: torch.device) -> None:\n",
    "    data_eval_src = data[idx][\"de_ids\"].to(device)\n",
    "    data_eval_tgt = data[idx][\"en_ids\"].to(device)\n",
    "    sentence_src = src_lang.decode(data_eval_src.detach().cpu().numpy().squeeze())\n",
    "    sentence_tgt = tgt_lang.decode(data_eval_tgt.detach().cpu().numpy().squeeze())\n",
    "    sentence_evaluated = translate_from_tensor(model, data_eval_src.unsqueeze(0), tgt_lang,\n",
    "                                               data_eval_tgt.unsqueeze(0))\n",
    "\n",
    "    print(f\"SOURCE: {sentence_src}\")\n",
    "    print(f\"TARGET: {sentence_tgt}\")\n",
    "    print(f\"MODEL: {sentence_evaluated}\")\n",
    "\n",
    "def evaluate_model(model: nn.Module,\n",
    "                   data_loader: torch.utils.data.DataLoader,\n",
    "                   loss_function: nn.NLLLoss,\n",
    "                   device: torch.device) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(data_loader):\n",
    "            loss = run_batch(model=model, loss_function=loss_function, batch=batch, device=device)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model=translator,\n",
    "          src=train_data[45][\"de_ids\"].reshape(1, -1).to(device),\n",
    "          src_lang=src_lang,\n",
    "          tgt_lang=tgt_lang,\n",
    "          max_tgt_length=100,\n",
    "          device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print untrained model translations\n",
    "print_sentences(data=train_data, idx=45, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 4.19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:29<00:00, 35.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 1, elapsed: 90 sec, train loss: 4.1816, validation loss: 3.4241\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 3.35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:55<00:00, 27.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 2, elapsed: 116 sec, train loss: 3.3527, validation loss: 2.9846\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 3.10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:49<00:00, 29.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 3, elapsed: 110 sec, train loss: 3.0965, validation loss: 2.8087\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:41<00:00, 31.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 4, elapsed: 102 sec, train loss: 2.9619, validation loss: 2.6707\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:46<00:00, 30.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 5, elapsed: 106 sec, train loss: 2.8722, validation loss: 2.5979\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:43<00:00, 31.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 6, elapsed: 103 sec, train loss: 2.8084, validation loss: 2.5369\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:43<00:00, 31.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 7, elapsed: 104 sec, train loss: 2.7588, validation loss: 2.4800\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:53<00:00, 28.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 8, elapsed: 113 sec, train loss: 2.7190, validation loss: 2.4401\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:36<00:00, 33.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 9, elapsed: 96 sec, train loss: 2.6826, validation loss: 2.3978\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:58<00:00, 27.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 10, elapsed: 119 sec, train loss: 2.6490, validation loss: 2.3522\n",
      "SOURCE: die tatsache, dass es hier mehrere richtige antworten auf die frage : \" was ist essen?\" gibt,\n",
      "TARGET: the fact that there are many right answers to the question, \" what is food?\"\n",
      "MODEL: the fact that there are several right answers to the question, \" what is eat?\" <EOS>?\"\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:38<00:00, 32.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 11, elapsed: 99 sec, train loss: 2.6209, validation loss: 2.3301\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:35<00:00, 33.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 12, elapsed: 95 sec, train loss: 2.5975, validation loss: 2.3024\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:33<00:00, 34.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 13, elapsed: 94 sec, train loss: 2.5793, validation loss: 2.2909\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [02:00<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 14, elapsed: 121 sec, train loss: 2.5628, validation loss: 2.2769\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:48<00:00, 29.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 15, elapsed: 109 sec, train loss: 2.5491, validation loss: 2.2543\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:35<00:00, 33.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 16, elapsed: 95 sec, train loss: 2.5384, validation loss: 2.2440\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:45<00:00, 30.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 17, elapsed: 106 sec, train loss: 2.5275, validation loss: 2.2356\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:28<00:00, 36.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 18, elapsed: 88 sec, train loss: 2.5187, validation loss: 2.2166\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [01:33<00:00, 34.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 19, elapsed: 94 sec, train loss: 2.5095, validation loss: 2.2034\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 3200 batches: 2.50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3221/3221 [02:03<00:00, 26.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 20, elapsed: 123 sec, train loss: 2.5014, validation loss: 2.1989\n",
      "SOURCE: mit haÌˆufigen aÌˆrgerlichen und negativen gedanken setzt man sich der groÃŸen gefahr aus, eine klinische depression, alkoholismus, essstoÌˆrungen und sogar herz - kreislauf - erkrankungen zu entwickeln.\n",
      "TARGET: because by spending so much time focused on upsetting and negative thoughts, you are actually putting yourself at significant risk for developing clinical depression, alcoholism, eating disorders, and even cardiovascular disease.\n",
      "MODEL: so of a a much more in on theset,, negative thoughts of the ' the looking the into large risk, depression aical depression, alcoholism, al disorders. and even heartiovascular disease. <EOS>.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "translator.train()\n",
    "\n",
    "model_name = f\"translator_transformer_v4_{num_layers}_layers\"\n",
    "best_val_loss = float(\"inf\")\n",
    "train_losses, valid_losses, all_losses = [], [], []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    epoch_losses = train_one_epoch(translator, optimizer,\n",
    "                                 loss_function, train_data_loader, device,\n",
    "                                 scheduler=scheduler)\n",
    "\n",
    "    epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    time_passed_seconds = time.time() - time_start\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    all_losses += epoch_losses\n",
    "\n",
    "    valid_loss = evaluate_model(model=translator, data_loader=valid_data_loader, loss_function=loss_function, device=device)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_val_loss:\n",
    "        # save best validaiton loss model\n",
    "        best_val_loss = valid_loss\n",
    "        print(\"Saving model state...\")\n",
    "        torch.save(translator.state_dict(), f\"{PARENT_DIR}/models/{model_name}_bestval.pt\")\n",
    "\n",
    "    # save model\n",
    "    torch.save(translator.state_dict(), f\"{PARENT_DIR}/models/{model_name}.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, elapsed: {time_passed_seconds:.0f} sec, train loss: {epoch_loss:.4f}, validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        random_eval_idx = int(np.random.choice(list(range(len(train_data)))))\n",
    "        print_sentences(data=train_data, idx=random_eval_idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)\n",
    "\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Average loss per epoch')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, color=\"blue\", label=\"Train\")\n",
    "plt.plot(valid_losses, color=\"red\", label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Average loss per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranslatorModel(\n",
       "  (encoder): Encoder(\n",
       "    (token_embedding): Embedding(4000, 256)\n",
       "    (positional_embedding): Embedding(128, 256)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (ff): PositionWiseFeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mha): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_embedding): Embedding(4000, 256)\n",
       "    (positional_embedding): Embedding(128, 256)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (fc): Linear(in_features=256, out_features=4000, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x DecoderLayer(\n",
       "        (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (ff): PositionWiseFeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (enc_attention): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.load_state_dict(torch.load(f\"{PARENT_DIR}/models/{model_name}_bestval.pt\", map_location=device))\n",
    "translator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: vor einigen jahren saÃŸ ich in einem flugzeug auf dem weg zur ostkuÌˆste, und der mann in der reihe neben mir war der buÌˆrgermeister von charlotte, north carolina.\n",
      "TARGET: but i was on an airplane in the east some years ago, and the man sitting across the aisle from me was the mayor of charlotte, north carolina.\n",
      "MODEL: a a sat sitting a airplane on a east coast years ago, and the man in in the series seriespher of char, the mayor of charlotte, north carolina. <EOS> cart\n"
     ]
    }
   ],
   "source": [
    "random_eval_idx = int(np.random.choice(list(range(len(train_data)))))\n",
    "print_sentences(data=train_data, idx=random_eval_idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: ein wort zur diashow. ich aktualisiere sie jedes mal, bevor ich sie zeige.\n",
      "TARGET: now, the slide show. i update the slide show every time i give it.\n",
      "MODEL: a, a wordsh. i ' everyate them di every you time before show you. <EOS> show\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SOURCE: sehen sie sich das an -- die duÌˆnne schwarze linie ist die hoÌˆchste geschwindigkeit, mit der wir je geflogen sind. und die rote line, das sind die schnellsten militaÌˆrjets und die blaue linie steht fuÌˆr die kommerzielle luftfahrt.\n",
      "TARGET: if you look at what happened -- this little black line is as fast as man ever flew, and the red line is top - of - the - line military fighters and the blue line is commercial air transport.\n",
      "MODEL: look you look at this -- -- the is black line is the theest we, flew, and the red line is the of line - line - line military projects and the blue line is the air travelation <EOS> blue\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SOURCE: und wir fuhren selbst.\n",
      "TARGET: driving ourselves.\n",
      "MODEL: and ourselves. <EOS> we\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SOURCE: ich erzaÌˆhle ihnen mal eine geschichte, dann verstehen sie mich vielleicht besser.\n",
      "TARGET: i ' ll tell you one quick story to illustrate what that ' s been like for me.\n",
      "MODEL: i ' ll tell you a story story, youustrate me you might s better better. me. <EOS>.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SOURCE: christ anderson : oh ja. dp : halt das, und lass nicht los. sie haben vielleicht das schon mal gesehen :\n",
      "TARGET: chris anderson : oh yeah. dp : hold that and don ' t let go.\n",
      "MODEL: christerson : oh yeah. dp : hold that, don ' t let go. you don\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idxs = [42, 422, 10, 7, 999]\n",
    "\n",
    "for idx in idxs:\n",
    "    print_sentences(data=train_data, idx=idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201c09cf1dd442b1a0e62014cfde161e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216475f7a1704a5bb6453e09254451d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d7a2251a20494497ca434d04a3c507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def get_tokenizer_fn(tgt_tokenizer: Tokenizer):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = tgt_tokenizer.encode(s).tokens\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn\n",
    "\n",
    "tokenizer_fn = get_tokenizer_fn(tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8079/8079 [06:37<00:00, 20.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.3048373443181313,\n",
       " 'precisions': [0.6653356099985951,\n",
       "  0.420663664562934,\n",
       "  0.2674592085632415,\n",
       "  0.18063284606631888],\n",
       " 'brevity_penalty': 0.8939447068629103,\n",
       " 'length_ratio': 0.8991905312563772,\n",
       " 'translation_length': 185066,\n",
       " 'reference_length': 205814}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute BLEU metric on test data\n",
    "predictions, references = [], []\n",
    "for idx in tqdm(range(test_data.num_rows)):\n",
    "    data_eval_src = test_data[idx][\"de_ids\"].reshape(1, -1).to(device)\n",
    "    sentence_evaluated = translate(model=translator,\n",
    "                                   src=data_eval_src,\n",
    "                                   src_lang=src_lang,\n",
    "                                   tgt_lang=tgt_lang,\n",
    "                                   max_tgt_length=max_length,\n",
    "                                   device=device,\n",
    "                                   clean=True)\n",
    "\n",
    "\n",
    "    predictions.append(sentence_evaluated)\n",
    "    references.append(test_data[idx][\"translation\"][\"en\"])\n",
    "\n",
    "bleu.compute(predictions=predictions, references=references, tokenizer=tokenizer_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
