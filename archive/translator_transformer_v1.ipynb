{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "from collections.abc import Callable\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "from spacy.language import Language\n",
    "from torch import Tensor, nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # for Apple chips\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', \"'re\", 'gon', 'na', 'go', 'swimming']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"We're gonna go swimming\"\n",
    "\n",
    "[token.text for token in en_nlp.tokenizer(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29000/29000 [00:01<00:00, 16536.46 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:00<00:00, 16902.50 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 17104.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_example(example: dict[str, str],\n",
    "                     en_nlp: Language,\n",
    "                     de_nlp: Language,\n",
    "                     max_length: int,\n",
    "                     lower: bool,  # noqa: FBT001\n",
    "                     bos_token: str,\n",
    "                     eos_token: str) -> dict[str, list[str]]:\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "\n",
    "    en_tokens = [bos_token, *en_tokens, eos_token]\n",
    "    de_tokens = [bos_token, *de_tokens, eos_token]\n",
    "\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n",
    "\n",
    "max_length = 1000\n",
    "lower = True\n",
    "sos_token = \"<BOS>\"  # noqa: S105\n",
    "eos_token = \"<EOS>\"  # noqa: S105\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"bos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our own `Language` object that will adapt to a text corpus and store vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self, name: str = None, min_count: int = 2) -> None:\n",
    "        self.name = name\n",
    "        self._min_count = min_count\n",
    "\n",
    "        self.index_to_word = {0: \"<BOS>\", 1: \"<PAD>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.word_counts = Counter(self.index_to_word.values())\n",
    "\n",
    "        self._last_index = list(self.index_to_word.keys())[-1]  # 3\n",
    "        self._max_seq_length = 0\n",
    "\n",
    "    def __repr__(self) -> None:\n",
    "        return f\"Language({self.name})\"\n",
    "\n",
    "    @property\n",
    "    def max_seq_length(self) -> int:\n",
    "        return self._max_seq_length\n",
    "\n",
    "    @property\n",
    "    def word_to_index(self) -> dict[str, int]:\n",
    "        return {word: index for index, word in self.index_to_word.items()}\n",
    "\n",
    "    @property\n",
    "    def words(self) -> list[str]:\n",
    "        return list(self.index_to_word.values())\n",
    "\n",
    "    @property\n",
    "    def num_words(self) -> int:\n",
    "        return len(self.words)\n",
    "\n",
    "    def get_word(self, idx: int) -> str:\n",
    "        return self.index_to_word.get(idx, \"<UNK>\")\n",
    "\n",
    "    def get_idx(self, word: str) -> int:\n",
    "        return self.word_to_index.get(word, self.word_to_index[\"<UNK>\"])\n",
    "\n",
    "    def string(self, idxs: list[int] | Tensor) -> str:\n",
    "        if isinstance(idxs, Tensor):\n",
    "            idxs = idxs.cpu().detach().numpy().squeeze()\n",
    "\n",
    "        return \" \".join([self.get_word(idx) for idx in idxs])\n",
    "\n",
    "    def indices(self, sentence: str | list[str]) -> list[int]:\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        return [self.get_idx(word) for word in sentence]\n",
    "\n",
    "    def add_word(self, word: str) -> None:\n",
    "        if word not in self.words:\n",
    "            self.word_counts.update([word])\n",
    "\n",
    "            if self.word_counts[word] > self._min_count:\n",
    "                # only update frequent words\n",
    "                self.index_to_word[self._last_index + 1] = word\n",
    "                self._last_index += 1\n",
    "\n",
    "    def add_sentence(self, sentence: list[str]) -> None:\n",
    "        seq_length = len(sentence)\n",
    "        if seq_length > self._max_seq_length:\n",
    "            self._max_seq_length = seq_length\n",
    "\n",
    "        for word in sentence:\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max seq length src: 46\n",
      "Max seq length tgt: 43\n"
     ]
    }
   ],
   "source": [
    "min_word_counts = 2\n",
    "tgt_lang = Language(\"English\", min_count=min_word_counts)\n",
    "src_lang = Language(\"German\", min_count=min_word_counts)\n",
    "\n",
    "NUM_EXAMPLES = train_data.num_rows\n",
    "\n",
    "# build vocabs\n",
    "for example in train_data.take(NUM_EXAMPLES):\n",
    "    src_lang.add_sentence(example[\"de_tokens\"])\n",
    "    tgt_lang.add_sentence(example[\"en_tokens\"])\n",
    "\n",
    "print(f\"Max seq length src: {src_lang.max_seq_length}\")\n",
    "print(f\"Max seq length tgt: {tgt_lang.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size src: 5374\n",
      "Vocab size tgt: 4556\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size src: {src_lang.num_words}\")\n",
    "print(f\"Vocab size tgt: {tgt_lang.num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def numericalize_example(example, tgt_lang: Language, src_lang: Language):\n",
    "#     en_ids = tgt_lang.indices(example[\"en_tokens\"])\n",
    "#     de_ids = src_lang.indices(example[\"de_tokens\"])\n",
    "#     return {\"en_ids\": en_ids, \"de_ids\": de_ids}\n",
    "\n",
    "# fn_kwargs = {\"tgt_lang\": tgt_lang, \"src_lang\": src_lang}\n",
    "\n",
    "# train_data = train_data.take(NUM_EXAMPLES).map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "# valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "# test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "\n",
    "# train_data.save_to_disk(\"datasets/train_data_de_en_v2.dataset\")\n",
    "# valid_data.save_to_disk(\"datasets/valid_data_de_en_v2.dataset\")\n",
    "# test_data.save_to_disk(\"datasets/test_data_de_en_v2.dataset\")\n",
    "\n",
    "# either load from disk or uncomment code above and run\n",
    "train_data = datasets.load_from_disk(\"datasets/train_data_de_en_v2.dataset\")\n",
    "valid_data = datasets.load_from_disk(\"datasets/valid_data_de_en_v2.dataset\")\n",
    "test_data = datasets.load_from_disk(\"datasets/test_data_de_en_v2.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> ein kleines mädchen klettert in ein <UNK> aus holz . <EOS>\n",
      "<BOS> a little girl climbing into a wooden playhouse . <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(src_lang.string(train_data[2][\"de_ids\"]))\n",
    "print(tgt_lang.string(train_data[2][\"en_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_collate_fn(pad_index: int) -> Callable[[dict], dict]:\n",
    "    def collate_fn(batch: dict) -> dict[str, int]:\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        return {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset: datasets.Dataset, batch_size: int, pad_index: int, *,\n",
    "                    shuffle: bool = True, pin_memory: bool = False) -> torch.utils.data.DataLoader:\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, tgt_lang.word_to_index[\"<PAD>\"])\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, tgt_lang.word_to_index[\"<PAD>\"])\n",
    "test_data_loader = get_data_loader(test_data, batch_size, tgt_lang.word_to_index[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, dropout_ratio: float = 0.1):\n",
    "        if hidden_dim % num_heads != 0:\n",
    "            msg = \"hidden_dim must be divisible by num_heads\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scaling = 1 / (self.head_dim ** .5)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            q: query of size (batch_size, seq_length, hidden_dim)\n",
    "            k: query of size (batch_size, seq_length, hidden_dim)\n",
    "            v: query of size (batch_size, seq_length, hidden_dim)\n",
    "            mask: optional mask of size (batch_size, 1, 1, seq_length)\n",
    "                  or (batch_size, 1, seq_length, seq_length)\n",
    "        Outputs\n",
    "            attention weighted embedding vectors of size (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # all Q, K, V are of shape (batch_size, seq_length, hidden_dim)\n",
    "        Q = self.fc_q(q)\n",
    "        K = self.fc_k(k)\n",
    "        V = self.fc_v(v)\n",
    "\n",
    "        batch_size, seq_length, _ = Q.size()\n",
    "\n",
    "        # all Q, K, V are of shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # energy.shape (batch_size, num_heads, seq_length, seq_length)\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) * self.scaling  # type: Tensor\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # attention.shape (batch_size, num_heads, seq_length, seq_length)\n",
    "        attention = energy.softmax(dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # x.shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, num_heads, head_dim)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, hidden_dim)\n",
    "        x = x.reshape(batch_size, seq_length, self.hidden_dim)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, hidden_dim)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, ff_dim: int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.fc_2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        # x.shape (batch_size, seq_length, emb_dim)\n",
    "        x = self.fc_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, ff_dim:int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.ff = PositionWiseFeedForward(hidden_dim=hidden_dim, ff_dim=ff_dim)\n",
    "\n",
    "        self.mha = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                      num_heads=num_heads,\n",
    "                                      dropout_ratio=dropout_ratio)\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            input of size (batch_size, seq_length, hidden_dim)\n",
    "        Outputs\n",
    "            (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # x1.shape (batch_size, seq_length, hidden_dim)\n",
    "        x1 = self.mha(src, src, src, mask)  # type: Tensor\n",
    "        x1 = self.norm_1(self.dropout(x1) + src)\n",
    "\n",
    "        # x2.shape (batch_size, seq_length, hidden_dim)\n",
    "        x2 = self.ff(x1)\n",
    "        x2 = self.norm_2(x1 + self.dropout(x2))\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int,\n",
    "                 num_heads: int, num_layers: int, ff_dim: int, max_seq_length: int,\n",
    "                 device: torch.device, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.scaling = hidden_dim ** (0.5)\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim=hidden_dim,\n",
    "                                                  num_heads=num_heads, ff_dim=ff_dim,\n",
    "                                                  dropout_ratio=dropout_ratio)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            input of shape (batch_size, seq_legth)\n",
    "        Outputs\n",
    "            encoded sequence of shape (batch_size, seq_legth, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = src.size()\n",
    "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "        x = self.token_embedding(src) * self.scaling + self.positional_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int,\n",
    "                 ff_dim:int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_3 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.ff = PositionWiseFeedForward(hidden_dim=hidden_dim, ff_dim=ff_dim)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                      num_heads=num_heads, dropout_ratio=dropout_ratio)\n",
    "        self.enc_attention = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                      num_heads=num_heads, dropout_ratio=dropout_ratio)\n",
    "\n",
    "    def forward(self, dec_input: Tensor, enc_outputs: Tensor,\n",
    "                enc_mask: Tensor, dec_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_input of shape (batch_size, seq_length, hidden_dim)\n",
    "            enc_input of shape (batch_size, seq_length, hidden_dim)\n",
    "            enc_mask of shape (batch_size, 1, 1, seq_length)\n",
    "            dec_mask of shape (batch_size, 1, seq_length, seq_length)\n",
    "        Outputs\n",
    "            (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # x1.shape (batch_size, seq_length, hidden_dim)\n",
    "        x1 = self.self_attention(dec_input, dec_input, dec_input, dec_mask)\n",
    "        x1 = self.norm_1(dec_input + self.dropout(x1))\n",
    "\n",
    "        # x2.shape (batch_size, seq_length, hidden_dim)\n",
    "        x2 = self.enc_attention(x1, enc_outputs, enc_outputs, enc_mask)\n",
    "        x2 = self.norm_2(x1 + self.dropout(x2))\n",
    "        x2 = self.norm_3(self.ff(x2))\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int,\n",
    "                 num_heads: int, num_layers: int, ff_dim: int, max_seq_length: int,\n",
    "                 device: torch.device, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.scaling = hidden_dim ** (0.5)\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim=hidden_dim,\n",
    "                                                  num_heads=num_heads, ff_dim=ff_dim,\n",
    "                                                  dropout_ratio=dropout_ratio)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, dec_input: Tensor, enc_outputs: Tensor,\n",
    "                enc_mask: Tensor, dec_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_inputs of shape (batch_size, seq_legth)\n",
    "            enc_outputs of shape (batch_size, seq_legth, hidden_dim)\n",
    "            dec_mask of shape (batch_size, 1, seq_length, seq_length)\n",
    "            enc_mask of shape (batch_size, 1, 1, seq_length)\n",
    "        Outputs\n",
    "            log-probabilities of shape (batch_size, seq_legth, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = dec_input.size()\n",
    "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "        x = self.token_embedding(dec_input) * self.scaling + self.positional_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "            x = layer(x, enc_outputs, enc_mask, dec_mask)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, vocab_size)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorModel(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, enc_pad_token: int,\n",
    "                 dec_pad_token: int, device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.enc_pad_token = enc_pad_token\n",
    "        self.dec_pad_token = dec_pad_token\n",
    "        self.device = device\n",
    "\n",
    "    def _create_enc_mask(self, enc_inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            enc_inputs of shape (batch_size, seq_length)\n",
    "        Outputs\n",
    "            mask with 0s for PAD tokens of shape (batch_size, 1, 1, seq_length)\n",
    "        \"\"\"\n",
    "        return (enc_inputs != self.enc_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def _create_dec_mask(self, dec_inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_inputs of shape (batch_size, seq_length)\n",
    "        Outputs\n",
    "            masks PAD tokens and future tokens; shape (batch_size, 1, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "        # mask_1.shape (batch_size, 1, 1, seq_length)\n",
    "        mask_1 = (dec_inputs != self.dec_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        batch_size, seq_length = dec_inputs.size()\n",
    "\n",
    "        # mask_2.shape (seq_length, seq_length)\n",
    "        mask_2 = torch.tril(torch.ones((seq_length, seq_length), device= self.device)).bool()\n",
    "\n",
    "        #mask_2.shape (batch_size, 1, seq_length, seq_length)\n",
    "        mask_2 = mask_2.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "        mask = mask_1 & mask_2\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            src of shape (batch_size, src_seq_length)\n",
    "            tgt of shape (batch_size, tgt_seq_length)\n",
    "        Outputs\n",
    "            decoded sequence of shape (batch_size, seq_length, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        enc_mask = self._create_enc_mask(src)\n",
    "        dec_mask = self._create_dec_mask(tgt)\n",
    "\n",
    "        enc_outputs = self.encoder(src, enc_mask)\n",
    "        dec_outputs = self.decoder(tgt, enc_outputs, enc_mask, dec_mask)\n",
    "\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num parameters: 7,717,836\n"
     ]
    }
   ],
   "source": [
    "def init_weights(model: nn.Module) -> None:\n",
    "    if hasattr(model, \"weight\") and model.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(model.weight.data)\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "loss_function = nn.NLLLoss(ignore_index=tgt_lang.get_idx(\"<PAD>\"))\n",
    "\n",
    "dropout_ratio = 0.3\n",
    "hidden_dim = 256\n",
    "ff_dim = 512\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "encoder = Encoder(vocab_size=src_lang.num_words,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  num_heads=num_heads,\n",
    "                  num_layers=num_layers,\n",
    "                  ff_dim=ff_dim,\n",
    "                  max_seq_length=100,\n",
    "                  device=device,\n",
    "                  dropout_ratio=dropout_ratio).to(device)\n",
    "\n",
    "decoder = Decoder(vocab_size=tgt_lang.num_words,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  num_heads=num_heads,\n",
    "                  num_layers=num_layers,\n",
    "                  ff_dim=ff_dim,\n",
    "                  max_seq_length=100,\n",
    "                  device=device,\n",
    "                  dropout_ratio=dropout_ratio).to(device)\n",
    "\n",
    "translator = TranslatorModel(encoder=encoder,\n",
    "                             decoder=decoder,\n",
    "                             enc_pad_token=src_lang.get_idx(\"<PAD>\"),\n",
    "                             dec_pad_token=tgt_lang.get_idx(\"<PAD>\"),\n",
    "                             device=device).to(device)\n",
    "\n",
    "translator.apply(init_weights)\n",
    "print(f\"Model num parameters: {count_parameters(translator):,}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(translator.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(model: nn.Module,\n",
    "              loss_function: nn.NLLLoss,\n",
    "              batch: dict[str, Tensor],\n",
    "              device: torch.device) -> float:\n",
    "    # src.shape (batch_size, seq_length)\n",
    "    src = batch[\"de_ids\"].to(device).transpose(1, 0)  # type: Tensor\n",
    "    tgt = batch[\"en_ids\"].to(device).transpose(1, 0)  # type: Tensor\n",
    "\n",
    "    log_probs = model(src, tgt[:, :-1])  # type: Tensor\n",
    "    log_probs = log_probs.reshape(-1, log_probs.size(-1))\n",
    "\n",
    "    loss = loss_function(log_probs, tgt[:, 1:].reshape(-1).long())\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    loss_function: nn.NLLLoss,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    device: torch.device) -> float:\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = run_batch(model=model, loss_function=loss_function, batch=batch, device=device)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(translator.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "def translate_from_tensor(model: nn.Module, src: Tensor,\n",
    "                          tgt_lang: Language, tgt: Tensor) -> str:\n",
    "    # outputs.shape (batch_size, tgt_seq_length, tgt_vocab_size)\n",
    "    log_probs = model(src, tgt=tgt)  # type: Tensor\n",
    "\n",
    "    # pred_top2.shape (batch_size, tgt_seq_length, 2)\n",
    "    _, pred_top2 = log_probs.topk(2, dim=-1)\n",
    "\n",
    "    # pred_top2.shape (tgt_seq_length, 2)\n",
    "    pred_top2 = pred_top2.squeeze(0)  # because batch_size=1 here\n",
    "\n",
    "    # unpack first 2 top predictions\n",
    "    first_pred, second_pred = pred_top2[:, 0].unsqueeze(1), pred_top2[:, 1].unsqueeze(1)\n",
    "\n",
    "    # in case first top prediction is UNK use second top prediction\n",
    "    unk_idx = tgt_lang.word_to_index[\"<UNK>\"]\n",
    "    indices = torch.where(first_pred == unk_idx, second_pred, first_pred)\n",
    "\n",
    "    indices = indices.squeeze().tolist()\n",
    "\n",
    "    sentence = tgt_lang.string(indices)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def translate(model: TranslatorModel, src: Tensor | str, src_lang: Language,\n",
    "              tgt_lang: Language, max_tgt_length: int, device: torch.device,\n",
    "              clean: bool = False) -> str:\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(src, str):\n",
    "        nlp = spacy.load(\"de_core_news_sm\")  # TODO: don't like this workaround\n",
    "        tokens = [token.text.lower() for token in nlp(src)]\n",
    "        tokens = [\"<BOS>\", *tokens, \"<EOS>\"]\n",
    "        src_idxs = np.array([src_lang.get_idx(word) for word in tokens])\n",
    "        src = torch.from_numpy(src_idxs).reshape(1, -1).to(device)  # (batch_size, src_seq_length)\n",
    "\n",
    "    enc_mask = model._create_enc_mask(src)  # noqa: SLF001\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(src, enc_mask)\n",
    "\n",
    "    tgt_indices = [tgt_lang.get_idx(\"<BOS>\")]\n",
    "\n",
    "    for _ in range(max_tgt_length):\n",
    "        tgt = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "        dec_mask = model._create_dec_mask(tgt)  # noqa: SLF001\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_probs = model.decoder(tgt, encoder_outputs, enc_mask, dec_mask)  # type: Tensor\n",
    "\n",
    "        # pred_top2.shape (batch_size, tgt_seq_length, 2)\n",
    "        _, pred_top2 = log_probs.topk(2, dim=-1)\n",
    "\n",
    "        # pred_top2.shape (tgt_seq_length, 2)\n",
    "        pred_top2 = pred_top2.squeeze(0)  # because batch_size=1 here\n",
    "\n",
    "        # unpack first 2 top predictions\n",
    "        first_pred, second_pred = pred_top2[-1, 0].item(), pred_top2[-1, 1].item()\n",
    "\n",
    "        # in case first top prediction is UNK use second top prediction\n",
    "        unk_idx = tgt_lang.word_to_index[\"<UNK>\"]\n",
    "        pred_token = second_pred if first_pred == unk_idx else first_pred\n",
    "\n",
    "        tgt_indices.append(pred_token)\n",
    "\n",
    "        if pred_token == tgt_lang.get_idx(\"<EOS>\"):\n",
    "            break\n",
    "\n",
    "    sentence = tgt_lang.string(tgt_indices)\n",
    "\n",
    "    if clean:\n",
    "        sentence = sentence.split(\"<BOS> \")[1]\n",
    "        sentence = sentence.split(\" <EOS>\")[0]\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def print_sentences(data: datasets.Dataset, idx: int, model: nn.Module,\n",
    "                    src_lang: Language, tgt_lang: Language, device: torch.device) -> None:\n",
    "    data_eval_src = data[idx][\"de_ids\"].to(device)\n",
    "    data_eval_tgt = data[idx][\"en_ids\"].to(device)\n",
    "    sentence_src = src_lang.string(data_eval_src)\n",
    "    sentence_tgt = tgt_lang.string(data_eval_tgt)\n",
    "    sentence_evaluated = translate_from_tensor(model, data_eval_src.unsqueeze(0), tgt_lang,\n",
    "                                               data_eval_tgt.unsqueeze(0))\n",
    "\n",
    "    print(f\"SOURCE: {sentence_src}\")\n",
    "    print(f\"TARGET: {sentence_tgt}\")\n",
    "    print(f\"MODEL: {sentence_evaluated}\")\n",
    "\n",
    "def evaluate_model(model: nn.Module,\n",
    "                   data_loader: torch.utils.data.DataLoader,\n",
    "                   loss_function: nn.NLLLoss,\n",
    "                   device: torch.device) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(data_loader):\n",
    "            loss = run_batch(model=model, loss_function=loss_function, batch=batch, device=device)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> jackets jackets jackets jackets jackets jackets jackets jackets jackets jackets jackets jackets jackets'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model=translator,\n",
    "          src=train_data[42][\"de_ids\"].reshape(1, -1).to(device),\n",
    "          src_lang=src_lang,\n",
    "          tgt_lang=tgt_lang,\n",
    "          max_tgt_length=13,\n",
    "          device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: <BOS> eine schöne braut geht auf einem gehweg mit ihrem neuen ehemann . <EOS>\n",
      "TARGET: <BOS> a beautiful bride walking on a sidewalk with her new husband . <EOS>\n",
      "MODEL: a middle bride is down the sidewalk with his son york . <EOS> .\n"
     ]
    }
   ],
   "source": [
    "# print untrained model translations\n",
    "print_sentences(data=train_data, idx=43, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training time on RTX 4090 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 75.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 1, elapsed: 3 sec, train loss: 4.5196, validation loss: 3.5590\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 76.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 2, elapsed: 3 sec, train loss: 3.4538, validation loss: 3.1130\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 76.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 3, elapsed: 3 sec, train loss: 3.0669, validation loss: 2.7754\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 76.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 4, elapsed: 3 sec, train loss: 2.7446, validation loss: 2.4658\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 76.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 5, elapsed: 3 sec, train loss: 2.4957, validation loss: 2.2505\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 76.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 6, elapsed: 3 sec, train loss: 2.3066, validation loss: 2.1258\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 76.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 7, elapsed: 3 sec, train loss: 2.1588, validation loss: 2.0305\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 76.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 8, elapsed: 3 sec, train loss: 2.0454, validation loss: 1.9702\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 78.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 9, elapsed: 3 sec, train loss: 1.9502, validation loss: 1.9108\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 71.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 10, elapsed: 3 sec, train loss: 1.8698, validation loss: 1.8726\n",
      "SOURCE: <BOS> ein schwarzer hund jagt einen ball im gras . <EOS>\n",
      "TARGET: <BOS> a black dog chases a ball in the grass . <EOS>\n",
      "MODEL: a black dog chases a ball in the grass . <EOS> .\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 71.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 11, elapsed: 3 sec, train loss: 1.8003, validation loss: 1.8378\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 71.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 12, elapsed: 3 sec, train loss: 1.7407, validation loss: 1.8075\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 76.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 13, elapsed: 3 sec, train loss: 1.6846, validation loss: 1.7723\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:02<00:00, 75.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, elapsed: 3 sec, train loss: 1.6359, validation loss: 1.7759\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 70.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 15, elapsed: 3 sec, train loss: 1.5921, validation loss: 1.7494\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 72.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 16, elapsed: 3 sec, train loss: 1.5494, validation loss: 1.7351\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 71.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 17, elapsed: 3 sec, train loss: 1.5110, validation loss: 1.7142\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 71.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, elapsed: 3 sec, train loss: 1.4786, validation loss: 1.7150\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 71.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, elapsed: 3 sec, train loss: 1.4489, validation loss: 1.7243\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:03<00:00, 71.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 20, elapsed: 3 sec, train loss: 1.4159, validation loss: 1.7129\n",
      "SOURCE: <BOS> ein mann demonstriert die verwendung einer <UNK> . <EOS>\n",
      "TARGET: <BOS> a man is <UNK> using a spray paint can . <EOS>\n",
      "MODEL: a man is showing a a mac paint . . <EOS> .\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "translator.train()\n",
    "\n",
    "model_name = f\"translator_transformer_{num_layers}_layers\"\n",
    "best_val_loss = float(\"inf\")\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    epoch_loss = train_one_epoch(translator, optimizer,\n",
    "                                 loss_function, train_data_loader, device)\n",
    "\n",
    "    time_passed_seconds = time.time() - time_start\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    valid_loss = evaluate_model(model=translator, data_loader=valid_data_loader, loss_function=loss_function, device=device)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_val_loss:\n",
    "        # save best validaiton loss model\n",
    "        best_val_loss = valid_loss\n",
    "        print(\"Saving model state...\")\n",
    "        torch.save(translator.state_dict(), f\"models/{model_name}_bestval.pt\")\n",
    "\n",
    "    # save model\n",
    "    torch.save(translator.state_dict(), f\"models/{model_name}.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, elapsed: {time_passed_seconds:.0f} sec, train loss: {epoch_loss:.4f}, validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        random_eval_idx = int(np.random.choice(list(range(NUM_EXAMPLES))))\n",
    "        print_sentences(data=train_data, idx=random_eval_idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)\n",
    "\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Average loss per epoch')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGzCAYAAAD9pBdvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqs0lEQVR4nO3dd1hT1+MG8DdhBBBwCyiICxQRcSvaulexWn511VG1Wu3AVutXa+1Sqq1a29paR7V1dFmrVu1SK+66WlRU3As3iBMEBAI5vz9OCYQdkhCSvJ/nuQ/Jzc255yRS3p57zrkKIYQAERERkZkozV0BIiIism0MI0RERGRWDCNERERkVgwjREREZFYMI0RERGRWDCNERERkVgwjREREZFYMI0RERGRWDCNERERkVgwjRFRiderUwahRo8xdDdLTqFGj4Orqau5qEBWKYYQoj8WLF0OhUKBt27bmrgoRkU1gGCHK48cff0SdOnXw77//4uLFi+auDhGR1WMYIcolNjYWBw4cwGeffYbq1avjxx9/LPM6aDQapKWllfl5qWipqanmrgKR1WIYIcrlxx9/ROXKldGnTx8MGDBAJ4yo1WpUqVIFL7zwQr73JSUlwcnJCZMnT9buS09Px/Tp09GgQQOoVCr4+PjgzTffRHp6us57FQoFxo8fjx9//BGBgYFQqVTYunUrAOCTTz5B+/btUbVqVTg7O6Nly5ZYv359vvM/fvwYr7/+OqpVqwY3Nzf069cPN2/ehEKhwIwZM3SOvXnzJkaPHg0PDw+oVCoEBgZixYoVpf7MLl++jIEDB6JKlSpwcXFBu3bt8Oeff+Y77ssvv0RgYCBcXFxQuXJltGrVCqtXr9a+/ujRI0ycOBF16tSBSqVCjRo10KNHDxw9erTI88+YMQMKhQJnz57FoEGD4O7ujqpVq2LChAkFhroffvgBLVu2hLOzM6pUqYLnnnsO169f1zmmc+fOaNKkCY4cOYKOHTvCxcUFb7/9dpH1OHv2LAYMGIAqVarAyckJrVq1wm+//aZzzKpVq6BQKLB371689NJLqFq1Ktzd3TFixAg8ePAgX5mLFy/W/puoWbMmwsPD8fDhw3zH/fPPPwgNDUXlypVRoUIFNG3aFF988UW+427evImwsDC4urqievXqmDx5MrKysopsF1GZEESk1ahRIzFmzBghhBB79+4VAMS///6rfX306NGiUqVKIj09Xed93377rQAgoqKihBBCZGVliZ49ewoXFxcxceJEsXTpUjF+/Hhhb28vnnnmGZ33AhABAQGievXqIiIiQixatEhER0cLIYTw9vYWr776qli4cKH47LPPRJs2bQQA8ccff+iUMWjQIAFAPP/882LRokVi0KBBIjg4WAAQ06dP1x4XHx8vvL29hY+Pj/jggw/EkiVLRL9+/QQAMX/+/GI/H19fXzFy5Eid8jw8PISbm5t45513xGeffSaCg4OFUqkUGzZs0B63bNkyAUAMGDBALF26VHzxxRdizJgx4vXXX9ceM3ToUOHo6CgmTZokvvnmGzF37lzRt29f8cMPPxRZp+nTpwsAIigoSPTt21csXLhQDB8+XPt55DZr1iyhUCjE4MGDxeLFi0VERISoVq2aqFOnjnjw4IH2uE6dOglPT09RvXp18dprr4mlS5eKTZs2FVqHkydPiooVK4rGjRuLuXPnioULF4qOHTsKhUKh8zmsXLlSW9cnn3xSLFiwQISHhwulUik6duwoNBpNvnZ1795dfPnll2L8+PHCzs5OtG7dWmRkZGiP27Ztm3B0dBS+vr5i+vTpYsmSJeL1118X3bt31x4zcuRI4eTkJAIDA8Xo0aPFkiVLRP/+/QUAsXjx4iI/X6KywDBC9J/Dhw8LACIyMlIIIYRGoxHe3t5iwoQJ2mP++usvAUD8/vvvOu8NDQ0V9erV0z7//vvvhVKpFH///bfOcV999ZUAIPbv36/dB0AolUpx6tSpfHVKTU3VeZ6RkSGaNGkiunbtqt135MgRAUBMnDhR59hRo0blCyNjxowRXl5e4u7duzrHPvfcc6JixYr5zpdX3jAyceJEAUCnnY8ePRJ169YVderUEVlZWUIIIZ555hkRGBhYZNkVK1YU4eHhRR5TkOw/2v369dPZ/+qrrwoA4vjx40IIIa5cuSLs7OzEhx9+qHNcTEyMsLe319nfqVMnAUB89dVXJapDt27dRFBQkEhLS9Pu02g0on379sLPz0+7LzuMtGzZUidQfPzxxwKA+PXXX4UQQiQkJAhHR0fRs2dP7WcohBALFy4UAMSKFSuEEEJkZmaKunXrCl9fX50wlX3+bCNHjhQAxAcffKBzTPPmzUXLli1L1EYiU+JlGqL//Pjjj/Dw8ECXLl0AyMsngwcPxpo1a7Rd2V27dkW1atXw888/a9/34MEDREZGYvDgwdp969atQ0BAABo1aoS7d+9qt65duwIAdu3apXPuTp06oXHjxvnq5OzsrHOexMREPPnkkzqXLrIv6bz66qs6733ttdd0ngsh8Msvv6Bv374QQujUq1evXkhMTCz2kkhemzdvRps2bfDEE09o97m6umLcuHG4cuUKTp8+DQCoVKkSbty4gaioqELLqlSpEv755x/cunVLrzpkCw8P13me3f7NmzcDADZs2ACNRoNBgwbptN3T0xN+fn75vhOVSlXgJbm87t+/j507d2LQoEF49OiRttx79+6hV69euHDhAm7evKnznnHjxsHBwUH7/JVXXoG9vb22rtu3b0dGRgYmTpwIpTLnP9Njx46Fu7u79jJYdHQ0YmNjMXHiRFSqVEnnHAqFIl9dX375ZZ3nTz75JC5fvlxsG4lMzd7cFSAqD7KysrBmzRp06dIFsbGx2v1t27bFp59+ih07dqBnz56wt7dH//79sXr1aqSnp0OlUmHDhg1Qq9U6YeTChQs4c+YMqlevXuD5EhISdJ7XrVu3wOP++OMPzJo1C8eOHdMZa5L7D83Vq1ehVCrzldGgQQOd53fu3MHDhw+xbNkyLFu2rET1Ks7Vq1cLnAIdEBCgfb1JkyaYOnUqtm/fjjZt2qBBgwbo2bMnhg4dig4dOmjf8/HHH2PkyJHw8fFBy5YtERoaihEjRqBevXolqoufn5/O8/r160OpVOLKlSsA5HcihMh3XLbc4QAAatWqBUdHx2LPe/HiRQgh8N577+G9994r8JiEhATUqlWr0Lq6urrCy8tLW9erV68CABo2bKhznKOjI+rVq6d9/dKlSwCAJk2aFFtPJyenfP8eK1euXOBYFaKyxjBCBGDnzp2Ii4vDmjVrsGbNmnyv//jjj+jZsycA4LnnnsPSpUuxZcsWhIWFYe3atWjUqBGCg4O1x2s0GgQFBeGzzz4r8Hw+Pj46z3P3gGT7+++/0a9fP3Ts2BGLFy+Gl5cXHBwcsHLlSp2BnyWl0WgAAMOHD8fIkSMLPKZp06Z6l1sSAQEBOHfuHP744w9s3boVv/zyCxYvXoz3338fERERAIBBgwbhySefxMaNG7Ft2zbMmzcPc+fOxYYNG/DUU0/pfc68PQMajQYKhQJbtmyBnZ1dvuPzLgpW0HdSkOzPdfLkyejVq1eBx+QNhuZQUJuJyguGESLIsFGjRg0sWrQo32sbNmzAxo0b8dVXX8HZ2RkdO3aEl5cXfv75ZzzxxBPYuXMn3nnnHZ331K9fH8ePH0e3bt0K7C4viV9++QVOTk7466+/oFKptPtXrlypc5yvry80Gg1iY2N1/o877xop1atXh5ubG7KystC9e/dS1SkvX19fnDt3Lt/+s2fPal/PVqFCBQwePBiDBw9GRkYGnn32WXz44YeYNm0anJycAABeXl549dVX8eqrryIhIQEtWrTAhx9+WKIwcuHCBZ3eoYsXL0Kj0aBOnToA5HcihEDdunXh7+9vSLN1ZPfcODg4lPhzvXDhgvZyIAAkJycjLi4OoaGhAHI+t3Pnzun0DGVkZCA2NlZ7nvr16wMATp48abTvlMgcOGaEbN7jx4+xYcMGPP300xgwYEC+bfz48Xj06JF2mqZSqcSAAQPw+++/4/vvv0dmZqbOJRpA/l/+zZs38fXXXxd4vpSUlGLrZWdnB4VCoTP18sqVK9i0aZPOcdn/N7548WKd/V9++WW+8vr3749ffvkFJ0+ezHe+O3fuFFunvEJDQ/Hvv//i4MGD2n0pKSlYtmwZ6tSpox0Hc+/ePZ33OTo6onHjxhBCQK1WIysrC4mJiTrH1KhRAzVr1sw3FboweYNkdvuzg8yzzz4LOzs7REREQAihc6wQIl8dS6pGjRro3Lkzli5diri4uHyvF/S5Llu2DGq1Wvt8yZIlyMzM1Na1e/fucHR0xIIFC3Tqunz5ciQmJqJPnz4AgBYtWqBu3br4/PPP8035zdtGovKMPSNk83777Tc8evQI/fr1K/D1du3aaRdAyw4dgwcPxpdffonp06cjKChIO0Yi2/PPP4+1a9fi5Zdfxq5du9ChQwdkZWXh7NmzWLt2Lf766y+0atWqyHr16dMHn332GXr37o2hQ4ciISEBixYtQoMGDXDixAntcS1btkT//v3x+eef4969e2jXrh327NmD8+fPA9C9XDFnzhzs2rULbdu2xdixY9G4cWPcv38fR48exfbt23H//n29Pru33noLP/30E5566im8/vrrqFKlCr799lvExsbil19+0Q6+7NmzJzw9PdGhQwd4eHjgzJkzWLhwIfr06QM3Nzc8fPgQ3t7eGDBgAIKDg+Hq6ort27cjKioKn376aYnqEhsbi379+qF37944ePAgfvjhBwwdOlR7+ax+/fqYNWsWpk2bhitXriAsLAxubm6IjY3Fxo0bMW7cOJ11YvSxaNEiPPHEEwgKCsLYsWNRr1493L59GwcPHsSNGzdw/PhxneMzMjLQrVs3DBo0COfOncPixYvxxBNPaP8NVq9eHdOmTUNERAR69+6Nfv36aY9r3bo1hg8fDkAG4yVLlqBv375o1qwZXnjhBXh5eeHs2bM4deoU/vrrr1K1h6jMmWsaD1F50bdvX+Hk5CRSUlIKPWbUqFHCwcFBOyVWo9EIHx8fAUDMmjWrwPdkZGSIuXPnisDAQKFSqUTlypVFy5YtRUREhEhMTNQeB6DQKa3Lly8Xfn5+QqVSiUaNGomVK1dqp7LmlpKSIsLDw0WVKlWEq6urCAsLE+fOnRMAxJw5c3SOvX37tggPDxc+Pj7CwcFBeHp6im7duolly5YV+1nlndorhBCXLl0SAwYMEJUqVRJOTk6iTZs2+dZBWbp0qejYsaOoWrWqUKlUon79+mLKlCnazyE9PV1MmTJFBAcHCzc3N1GhQgURHBxcojUwsj+P06dPiwEDBgg3NzdRuXJlMX78ePH48eN8x//yyy/iiSeeEBUqVBAVKlQQjRo1EuHh4eLcuXPaYzp16lTsVOS8Ll26JEaMGCE8PT2Fg4ODqFWrlnj66afF+vXrtcdkT+3ds2ePGDdunKhcubJwdXUVw4YNE/fu3ctX5sKFC0WjRo2Eg4OD8PDwEK+88kq+KbxCCLFv3z7Ro0cP7WfXtGlT8eWXX2pfHzlypKhQoUKhnx2RuSmEYF8ekTU6duwYmjdvjh9++AHDhg0zd3VMZsaMGYiIiMCdO3dQrVo1c1enSKtWrcILL7yAqKioYnvGiGwJx4wQWYHHjx/n2/f5559DqVSiY8eOZqgREVHJccwIkRX4+OOPceTIEXTp0gX29vbYsmULtmzZgnHjxuWbRkxEVN4wjBBZgfbt2yMyMhIzZ85EcnIyateujRkzZuSbckxEVB5xzAgRERGZFceMEBERkVkxjBAREZFZWcSYEY1Gg1u3bsHNza3US2sTERFR2RJC4NGjR6hZs6bOHajzsogwcuvWLc4IICIislDXr1+Ht7d3oa9bRBhxc3MDIBvj7u5utHLVajW2bduGnj175rt9uDWypfayrdbLltrLtlovW2lvUlISfHx8tH/HC2MRYST70oy7u7vRw4iLiwvc3d2t+h9DNltqL9tqvWypvWyr9bK19hY3xIIDWImIiMisGEaIiIjIrBhGiIiIyKwsYswIERFZByEEMjMzkZWVpbNfrVbD3t4eaWlp+V6zRtbSXjs7O9jb2xu87AbDCBERlYmMjAzExcUhNTU132tCCHh6euL69es2sZ6UNbXXxcUFXl5ecHR0LHUZDCNERGRyGo0GsbGxsLOzQ82aNeHo6KjzR1ij0SA5ORmurq5FLo5lLayhvUIIZGRk4M6dO4iNjYWfn1+p28IwQkREJpeRkQGNRgMfHx+4uLjke12j0SAjIwNOTk4W+8dZH9bSXmdnZzg4OODq1ava9pSG5X4CRERkcSz5Dy8VzBjfKf9VEBERkVkxjBAREZFZMYwQERGVoTp16uCLL74wdzXKFYYRIiKiAigUiiK3GTNmlKrcqKgojB071riVtXAGhZE5c+ZAoVBg4sSJhR6zatWqfF9gaUfbGtuyZUrMn98C166ZuyZERFTexMXFabfPP/8c7u7uOvsmT56sPTZ7MbeSqF69eoEzimxZqcNIVFQUli5diqZNmxZ7bN4v8OrVq6U9rVGtXKnAnj0++Ocfy15whojI0ggBpKSYZxOiZHX09PTUbhUrVoRCodA+P3v2LNzc3LBlyxa0bNkSKpUK+/btw6VLl/DMM8/Aw8MDrq6uaN26NbZv365Tbt7LNAqFAt988w3+7//+Dy4uLvDz88Nvv/1mzI+73CtVGElOTsawYcPw9ddfo3LlysUen/sL9PT0hIeHR2lOa3StWsl/kUeOMIwQEZWl1FTA1TVnc3dXwtu7EtzdlTr7TbEVsABsqb311luYM2cOzpw5g6ZNmyI5ORmhoaHYsWMHoqOj0bt3b/Tt2xfXiumCj4iIwKBBg3DixAmEhoZi2LBhuH//vvEqWs6VatGz8PBw9OnTB927d8esWbOKPT45ORm+vr7QaDRo0aIFPvroIwQGBhZ6fHp6OtLT07XPk5KSAMi1/NVqdWmqXKDgYA0AOxw+DKOWW15lt5FttS621FbAttprTW1Vq9UQQkCj0fy3AeYatphzfv3eU9DPGTNmoFu3btrjKlWqhKCgIO3ziIgIbNy4Eb/++ivCw8O1+8V/3TPZP0eOHInBgwcDAGbNmoUFCxbg0KFD6N27t56tK3sajQZCCKjVatjZ2em8VtJ/u3qHkTVr1uDo0aOIiooq0fENGzbEihUr0LRpUyQmJuKTTz5B+/btcerUKXh7exf4ntmzZyMiIiLf/m3bthn1OltGhhuAroiK0uCPPzbDVtbiiYyMNHcVygzbar1sqb3W0FZ7e3t4enoiOTkZGRkZEAK4ccM8dcnMBP77f9wSS0tLgxBC+z/H2ffXadiwoXYfIP/ne+7cudi2bRvi4+ORlZWFx48f48KFC9rjNBqN9n+4Hz16BABo0KCBTjlubm64du2azr7yKiMjA48fP8bevXvzjZsp6D5EBdErjFy/fh0TJkxAZGRkiQehhoSEICQkRPu8ffv2CAgIwNKlSzFz5swC3zNt2jRMmjRJ+zwpKQk+Pj7o2bMn3N3d9alykR4/VmPq1Ew8fuyABg1C0aiR0Youl9RqNSIjI9GjRw84ODiYuzomxbZaL1tqrzW1NS0tDdevX4erq6v270fFijmvCyHw6NEjuLm5lcsbxzk5OUGhUGj/BmX/j7Gnp6fO36WpU6di+/bt+Pjjj9GgQQM4Oztj0KBBOu9VKpVQqVQAZOgA5NjK3OUolUo4Ojoa9W+eqaSlpcHZ2RkdO3bMlw1KGqb0CiNHjhxBQkICWrRood2XlZWFvXv3YuHChUhPT8/XRZOXg4MDmjdvjosXLxZ6jEql0n5Red9r7F/IevUSceZMVRw75oBcPWtWzRSfY3nFtlovW2qvNbQ1KysLCoUCSqWywOXDsy97ZB9T3mTXqaCfuet74MABjBo1Cv379wcge0quXLmCzp076xyXHbiyfxb0uRT2WZU3SqUSCoWiwH+nJf13q1cru3XrhpiYGBw7dky7tWrVCsOGDcOxY8eKDSKA/AcZExMDLy8vfU5tMg0aPAQAlPCqExERUaH8/PywYcMGHDt2DMePH8fQoUO1QYsKp1fPiJubG5o0aaKzr0KFCqhatap2/4gRI1CrVi3Mnj0bAPDBBx+gXbt2aNCgAR4+fIh58+bh6tWrePHFF43UBMM0aPAAAHD4sJkrQkREFu+zzz7D6NGj0b59e1SrVg1Tp061iHEf5laq2TRFuXbtmk630oMHDzB27FjEx8ejcuXKaNmyJQ4cOIDGjRsb+9Slkt0zEh0NqNWAhfeEEhGRCYwaNQqjRo3SPu/cubN2JkxuderUwc6dO3X25Z5FAwBXrlyBRqPRhpSCynn48KHhlbYgBoeR3bt3F/l8/vz5mD9/vqGnMRkvrxRUrCiQmKjAqVNAs2bmrhEREZFtKf8jY0xMqQRatpSplJdqiIiIyp7NhxEAaNFChhEOYiUiIip7DCPIWRaeYYSIiKjsMYwg5zJNTAyQlmbmyhAREdkYhhEAtWsD1avLJYKPHzd3bYiIiGwLwwgAhQJo3Vo+5qUaIiKissUw8p9WreRPzqghIiIqWwwj/2HPCBERkXkwjPwnu2fkzBngvzs6ExERGaRz586YOHGi9nmdOnXw+eefF/kehUKBTZs2GXxuY5VTFhhG/uPpCXh7A0LIpeGJiMi29e3bF7179y7wtb///hsKhQInTpzQq8yoqCiMGzfOGNXTmjFjBpoVsHx4XFwcnnrqKaOey1QYRnLhpRoiIso2ZswYREZG4saNG/leW7lyJVq1aoWmTZvqVWb16tXh4uJirCoWydPTEyqVqkzOZSiGkVwYRoiIyogQQEqKebYCbkxXkKeffhrVq1fHqlWrdPYnJydj3bp1CAsLw5AhQ1CrVi24uLggKCgIP/30U5Fl5r1Mc+HCBXTs2BFOTk5o3LgxIiMj871n6tSp8Pf3h4uLC+rVq4f33nsParUaALBq1SpERETg+PHjUCgUUCgU2vrmvUwTExODrl27wtnZGVWrVsW4ceOQnJysfX3UqFEICwvDJ598Ai8vL1StWhXh4eHac5mS0e/aa8k4o4aIqIykpgKurtqnSgCVyurcyclAhQrFHmZvb48RI0Zg1apVeOedd6BQKAAA69atQ1ZWFoYPH45169Zh6tSpcHd3x59//onnn38e9evXR5s2bYotX6PRYMCAAfDw8MA///yDxMREnfEl2dzc3LBq1SrUrFkTMTExGDt2LNzc3PDmm29i8ODBOHnyJLZu3Yrt27cDACpWrJivjJSUFPTq1QshISGIiopCQkICXnzxRYwfP14nbO3atQteXl7YtWsXLl68iMGDB6NZs2YYO3Zsse0xBHtGcskOI5cuAffvm7cuRERkfqNHj8alS5ewZ88e7b6VK1eif//+8PX1xeTJk9GsWTPUq1cPr732Gnr37o21a9eWqOzdu3fj7Nmz+O677xAcHIyOHTvio48+ynfcu+++i/bt26NOnTro27cvJk+erD2Hs7MzXF1dYW9vD09PT3h6esLZ2TlfGatXr0ZaWhq+++47NGnSBF27dsXChQvx/fff4/bt29rjKleujIULF6JRo0Z4+umn0adPH+zYsUPfj01v7BnJpXJloEED4OJF2TvSs6e5a0REZKVcXGQPxX80Gg2SkpLg7u4OpdLE/5+sx5iNRo0aoX379lixYgU6d+6Mixcv4u+//8YHH3yArKwsfPTRR1i7di1u3ryJjIwMpKenl3hMyPnz5+Hj44OaNWtq94WEhOQ77ueff8aCBQtw6dIlJCcnIzMzE+7u7iVuAwCcOXMGwcHBqJCrR6hDhw7QaDQ4d+4cPDw8AACBgYGws7PTHuPl5YWYmBi9zlUa7BnJg5dqiIjKgEIhL5WYY/vvcktJjRkzBr/88gsePXqElStXon79+ujUqRPmzZuHL774AlOnTsWuXbtw7Ngx9OrVCxkZGUb7mA4ePIhhw4YhNDQUf/zxB6Kjo/HOO+8Y9Ry5OTg46DxXKBTQaDQmOVduDCN5cBArERHlNmjQICiVSqxevRrfffcdRo8eDYVCgf379+OZZ57B8OHDERwcjHr16uH8+fMlLtff3x/Xr19HXFycdt+hQ4d0jjlw4AB8fX3xzjvvoFWrVvDz88PVq1d1jnF0dERWVlaR5woICMDx48eRkpKi3bd//34olUo0bNiwxHU2FYaRPBhGiIgoN1dXVwwePBjTpk1DXFwcRo0aBQDw8/NDZGQkDhw4gDNnzuCll17SGX9RnM6dO8Pf3x8jR47E8ePH8ffff+Odd97ROcbPzw/Xrl3DmjVrcOnSJSxYsAAbN27UOaZOnTqIjY3FsWPHcPfuXaSnp+c717Bhw+Dk5ISRI0fi5MmT2LVrF1577TU8//zz2ks05sQwkkfz5oBSCdy8CeQKq0REZMPGjBmDBw8eoFevXtoxHu+++y5atGiBXr16oXPnzvD09ERYWFiJy1Qqlfjll1/w+PFjtGnTBi+++CI+/PBDnWP69euHN954A+PHj0ezZs1w4MABvPfeezrH9O/fH71790aXLl1QvXr1AqcXu7i44K+//sL9+/fRunVrDBgwAN26dcPChQv1/zBMgANY83B1BQICgFOn5LiRvn3NXSMiIjK3kJAQiDzrk1SpUqXY5dZ3796t8/zKlSsAoB2H4e/vj7///lvnmLzn+fjjj/Hxxx/r7Ms9BVilUmH9+vX5zp23nKCgIOzcubPQuuZdTwVAsUvXGwt7RgrASzVERERlh2GkAJxRQ0REVHYYRgqQu2ekhKsGExERUSkxjBSgaVPA3h64exe4ds3ctSEiIrJuDCMFcHKSgQTguBEiImPKO6iSLJ8xvlOGkUJwECsRkfFkr+yZmppq5pqQsWV/p3lXb9UHp/YWolUrYOlSDmIlIjIGOzs7VKpUCQkJCQDkuheKXMuyazQaZGRkIC0tzfT3pikHrKG9QgikpqYiISEBlSpV0rmnjb4YRgqR3TNy+DCg0ciF0IiIqPQ8PT0BQBtIchNC4PHjx3B2dtYJKdbKmtpbqVIl7XdbWgwjhQgMlGNHkpKACxeAcrB0PxGRRVMoFPDy8kKNGjWgVqt1XlOr1di7dy86duxoUHe/pbCW9jo4OBjUI5KNYaQQ9vZyafiDB2XvCMMIEZFx2NnZ5fsDZmdnh8zMTDg5OVn0H+eSsrX2FocXH4rAQaxERESmxzBSBIYRIiIi02MYKUL2svDR0UBmpnnrQkREZK0YRorg7w+4uwOPHwOnT5u7NkRERNaJYaQISiXQsqV8zEs1REREpsEwUgzewZeIiMi0GEaKwUGsREREpsUwUozsMHLiBJCebt66EBERWSOGkWL4+gJVqwJqtQwkREREZFwMI8VQKHiphoiIyJQYRkqAYYSIiMh0GEZKgDNqiIiITIdhpASye0ZOnwZSUsxbFyIiImtjUBiZM2cOFAoFJk6cWORx69atQ6NGjeDk5ISgoCBs3rzZkNOWOS8voFYtQKMBjh41d22IiIisS6nDSFRUFJYuXYqmTZsWedyBAwcwZMgQjBkzBtHR0QgLC0NYWBhOnjxZ2lObBS/VEBERmUapwkhycjKGDRuGr7/+GpUrVy7y2C+++AK9e/fGlClTEBAQgJkzZ6JFixZYuHBhqSpsLhzESkREZBr2pXlTeHg4+vTpg+7du2PWrFlFHnvw4EFMmjRJZ1+vXr2wadOmQt+Tnp6O9FwrjCUlJQEA1Go11Gp1aapcoOyySlJms2YKAPaIihJQqy3zFr76tNfSsa3Wy5bay7ZaL1tpb0nbp3cYWbNmDY4ePYqoEnYRxMfHw8PDQ2efh4cH4uPjC33P7NmzERERkW//tm3b4OLiol+FSyAyMrLYY5KSHACE4uJFBdaujYSrq+X+AypJe60F22q9bKm9bKv1svb2pqamlug4vcLI9evXMWHCBERGRsLJyalUFSuJadOm6fSmJCUlwcfHBz179oS7u7vRzqNWqxEZGYkePXrAwcGh2ONnzBC4fFmBqlV7ols3YbR6lBV922vJ2FbrZUvtZVutl620N/vKRnH0CiNHjhxBQkICWrRood2XlZWFvXv3YuHChUhPT4ednZ3Oezw9PXH79m2dfbdv34anp2eh51GpVFCpVPn2Ozg4mORLK2m5rVoBly8D0dH26N3b6NUoM6b6HMsjttV62VJ72VbrZe3tLWnb9BrA2q1bN8TExODYsWParVWrVhg2bBiOHTuWL4gAQEhICHbs2KGzLzIyEiEhIfqculzIHsTKGTVERETGo1fPiJubG5o0aaKzr0KFCqhatap2/4gRI1CrVi3Mnj0bADBhwgR06tQJn376Kfr06YM1a9bg8OHDWLZsmZGaUHY4o4aIiMj4jL4C67Vr1xAXF6d93r59e6xevRrLli1DcHAw1q9fj02bNuULNZagRQt547zr14E8V56IiIiolEo1tTe33bt3F/kcAAYOHIiBAwcaeiqzc3MDGjUCzpyRl2r69DF3jYiIiCwf702jJ16qISIiMi6GET1xWXgiIiLjYhjRU+6eEWF5S40QERGVOwwjegoOBuztgYQEOZCViIiIDMMwoidnZyB7IhAv1RARERmOYaQUOIiViIjIeBhGSoFhhIiIyHgYRkoh94waDmIlIiIyDMNIKTRpAjg5AYmJwMWL5q4NERGRZWMYKQUHB6BZM/mYl2qIiIgMwzBSSlz8jIiIyDgYRkqJg1iJiIiMg2GklLLDyNGjQGameetCRERkyRhGSsnfH3B1BVJTgbNnzV0bIiIiy8UwUkp2dkDLlvIxL9UQERGVHsOIAbIHsTKMEBERlR7DiAGyx41wRg0REVHpMYwYIDuMHD8OZGSYty5ERESWimHEAHXrAlWqyCBy4oS5a0NERGSZGEYMoFBw8TMiIiJDMYwYiIufERERGYZhxECcUUNERGQYhhEDZfeMnDolF0AjIiIi/TCMGKhWLcDLC9BogOhoc9eGiIjI8jCMGAEv1RAREZUew4gRcPEzIiKi0mMYMQLOqCEiIio9hhEjyL5Mc/488PChWatCRERkcRhGjKBaNaBOHfn46FGzVoWIiMjiMIwYCS/VEBERlQ7DiJFwWXgiIqLSYRgxEvaMEBERlQ7DiJG0bCl/Xr0K3Llj3roQERFZEoYRI3F3Bxo2lI95qYaIiKjkGEaMiJdqiIiI9McwYkQMI0RERPpjGDGi3DNqhDBvXYiIiCwFw4gRNWsG2NkB8fHAzZvmrg0REZFlYBgxIhcXIDBQPualGiIiopKx3TCi0UCxaRNCpk8HUlKMVizv4EtERKQf2w0jWVmwe+st1Dh+HMpvvjFasRzESkREpB/bDSMODsiaOhUAoPz0U+DxY6MUy0GsRERE+rHdMAJADBuG1OrVoYiPB4zUOxIUBDg6Ag8eAJcvG6VIIiIiq6ZXGFmyZAmaNm0Kd3d3uLu7IyQkBFu2bCn0+FWrVkGhUOhsTk5OBlfaaBwdcX7AAPl47lwgPd0YRaJZM/mYl2qIiIiKp1cY8fb2xpw5c3DkyBEcPnwYXbt2xTPPPINTp04V+h53d3fExcVpt6tXrxpcaWO63rUrhLe3nIu7cqVRysy+VMMwQkREVDy9wkjfvn0RGhoKPz8/+Pv748MPP4SrqysOHTpU6HsUCgU8PT21m4eHh8GVNiaNgwM0kyfLJ7NnAxkZBpfJGTVEREQlZ1/aN2ZlZWHdunVISUlBSEhIocclJyfD19cXGo0GLVq0wEcffYTA7MU4CpGeno70XJdMkpKSAABqtRpqtbq0Vc4nu6z055+H89y5UFy7hsyVKyFGjzaoXHmZxgFHjgikpWXCzs7gqhpFdnuN+RmWV2yr9bKl9rKt1stW2lvS9imE0G/OR0xMDEJCQpCWlgZXV1esXr0aoaGhBR578OBBXLhwAU2bNkViYiI++eQT7N27F6dOnYK3t3eh55gxYwYiIiLy7V+9ejVcXFz0qW6J1fvtNwStWIEUDw/sWLQIwr7UOQ1ZWcCwYX2QlmaPBQt2onbtR0asKRERkWVITU3F0KFDkZiYCHd390KP0zuMZGRk4Nq1a0hMTMT69evxzTffYM+ePWjcuHGx71Wr1QgICMCQIUMwc+bMQo8rqGfEx8cHd+/eLbIx+lKr1YiMjESPHj3goFbD3t8fioQEZH7zDcSIEQaV3bWrHfbtU+KbbzIxYkT5mOOr014HB3NXx6TYVutlS+1lW62XrbQ3KSkJ1apVKzaM6P2//46OjmjQoAEAoGXLloiKisIXX3yBpUuXFvteBwcHNG/eHBcvXizyOJVKBZVKVeD7TfGlOTg4wMHFBZgyBZgyBfZz5wIjRwIG9I60aQPs2wdER9tjzBgjVtYITPU5lkdsq/WypfayrdbL2ttb0rYZvM6IRqPR6cUoSlZWFmJiYuDl5WXoaU3j5ZeBatWACxeAn382qCjOqCEiIioZvcLItGnTsHfvXly5cgUxMTGYNm0adu/ejWHDhgEARowYgWnTpmmP/+CDD7Bt2zZcvnwZR48exfDhw3H16lW8+OKLxm2Fsbi6ApMmycezZsnBH6WUPaPm2DHg/n3Dq0ZERGSt9AojCQkJGDFiBBo2bIhu3bohKioKf/31F3r06AEAuHbtGuLi4rTHP3jwAGPHjkVAQABCQ0ORlJSEAwcOlGh8idmEhwOVKwNnzwLr15e6mPr1gSZN5EzhIobHEBER2Ty9BkUsX768yNd3796t83z+/PmYP3++3pUyK3d34I03gPfflyli4EBAqf/VLIUC+PRToFcvYOFC4JVXAH9/E9SXiIjIwtn0vWkK9dprQMWKwKlTwMaNpS6mZ08gNBTIzJRjY4mIiCg/hpGCVKoETJggH8+cadDtdz/5BLCzA377Ddi50zjVIyIisiYMI4WZMAFwcwOOHwd+/73UxQQEyEs0gLz6Y8CYWCIiIqvEMFKYKlWA8ePl4w8+MKh3ZMYM2dly4oTR7sVHRERkNRhGijJpElChAnDkCLBlS6mLqVpVjocFgHffBR5xdXgiIiIthpGiVKsGvPqqfGxg70h4OODnB9y+LW8OTERERBLDSHH+9z/A2Rn45x9g+/ZSF+PoCMybJx9/9hlw5YpxqkdERGTpGEaK4+Ehl4kHgIgIg3pH+vUDunQB0tOBt94yUv2IiIgsHMNISUyZAqhUwP79QJ6F3fShUMheEYVC3vpm/37jVZGIiMhSMYyUhJcXMHasfPzBBwYV1awZMHq0fPzGG4BGY1jViIiILB3DSElNnSoHfuzeDezda1BRs2bJe/JFRQGrVxunekRERJaKYaSkvL1zujQMvPOdpyeQfXPjadOA1FQD60ZERGTBGEb08dZbgL29nFVz8KBBRb3xBlC7NnDjhlwynoiIyFYxjOjD1xcYNUo+NrB3xNkZmDtXPp47F7h507CqERERWSqGEX1NmybvfLdlixz0YYDBg4GQEHmZ5p13jFQ/IiIiC8Mwoq969YDhw+VjA3tHFApg/nz5+Ntv5arzREREtoZhpDTefhtQKuXdfKOjDSqqbVtg2DD5+I03DFpTjYiIyCIxjJSGvz8wZIh8bGDvCCDvVePsDPz9N7Bhg8HFERERWRSGkdJ65x15nWXjRiAmxqCifHyAyZPl4zfflMvFExER2QqGkdIKCAAGDZKPZ80yuLg335QLvV6+DCxYYHBxREREFoNhxBDZU2DWrQNOnzaoKFdX4KOP5ONZs4CEBAPrRkREZCEYRgwRFAQ8+6wcdfrhhwYXN2IE0KIFkJQETJ9uhPoRERFZAIYRQ733nvy5Zg1w/rxBRSmVOVN9ly0DTp40sG5EREQWgGHEUM2aAf36ydvvZl9nMUDHjrKzRaMB/vc/TvUlIiLrxzBiDNm9Iz/8AFy6ZHBxH38sbxC8bZtc6JWIiMiaMYwYQ6tWQGgokJUlFw0xUP36wOuvy8f/+x+gVhtcJBERUbnFMGIs2b0j334LXLlicHHvvgtUqwacPQssXWpwcUREROUWw4ixtGsH9OgBZGYCc+YYXFzFisAHH8jH06cDDx4YXCQREVG5xDBiTO+/L3+uWAFcv25wcWPHAoGBwP37Rll1noiIqFxiGDGmJ54AunSRgzw+/tjg4uztgU8/lY8XLgQuXDC4SCIionKHYcTYsntHvv4auHXL4OJ69QKeekrmmylTDC6OiIio3GEYMbZOnWQPSXo6MG+eUYr89FPAzg749Vdg1y6jFElERFRuMIwYm0KR0zvy1VdAfLzBRQYEAC+/LB+/8YacQUxERGQtGEZMoXt3ObsmLS1n0IeBZswAKlUCjh8HVq0ySpFERETlAsOIKeTuHVm82CjrjlSrllPkO+8Ajx4ZXCQREVG5wDBiKr17yxvNpKYCL71klJvMhIcDfn7A7dtGWcqEiIioXGAYMRWFQs6ocXKSN5n59luDi3R0zBkT++mnwNWrBhdJRERkdgwjpuTvD0REyMdvvGGUwaz9+smlTNLTgalTDS6OiIjI7BhGTG3SJKBFC+DhQ2D8eIOLUyiAzz6TP3/+GThwwPAqEhERmRPDiKnZ28vl4e3tgV9+kZuBmjUDRo+Wj8PDeVdfIiKybAwjZSE4GHjrLfk4PFzebMZAH34IVKkCHDsmHxMREVkqhpGy8u67QKNGcirM//5ncHEeHnLWMADMmgUcOWJwkURERGbBMFJWVCpg+XI52GPVKjnDxkCDBwODBskVWUeMkGusERERWRq9wsiSJUvQtGlTuLu7w93dHSEhIdiyZUuR71m3bh0aNWoEJycnBAUFYfPmzQZV2KK1bw+89pp8PG4ckJxscJGLF8tektOngffeM7g4IiKiMqdXGPH29sacOXNw5MgRHD58GF27dsUzzzyDU6dOFXj8gQMHMGTIEIwZMwbR0dEICwtDWFgYTp48aZTKW6QPPwTq1JGLhLz9tsHFVa0qlzMB5Noj+/YZXCQREVGZ0iuM9O3bF6GhofDz84O/vz8+/PBDuLq64tChQwUe/8UXX6B3796YMmUKAgICMHPmTLRo0QILFy40SuUtkqsrsGyZfLxwIbB/v8FF9u0LvPCCXOR15EijdLgQERGVGfvSvjErKwvr1q1DSkoKQkJCCjzm4MGDmDRpks6+Xr16YdOmTUWWnZ6ejvT0dO3zpKQkAIBarYbaiPNYs8syZpkl0rkz7EaOhPLbbyHGjEFmVJRcqdUAH38MbN9uj8uXFZgyJQsLFmjyHWO29poB22q9bKm9bKv1spX2lrR9CiH0u2lKTEwMQkJCkJaWBldXV6xevRqhoaEFHuvo6Ihvv/0WQ4YM0e5bvHgxIiIicPv27ULPMWPGDERkr1yay+rVq+Hi4qJPdcsth+RkdH3tNTg9eIBzAwfi7LBhBpd5/Hh1TJ/eHgAwY8YBNGt2x+AyiYiISis1NRVDhw5FYmIi3N3dCz1O756Rhg0b4tixY0hMTMT69esxcuRI7NmzB40bNzaowrlNmzZNp0clKSkJPj4+6NmzZ5GN0ZdarUZkZCR69OgBBwcHo5VbUgpHR2DQIPhv2IB6U6bI1cwMEBoKxMdnYckSO3zzTQiiozNRsWLO6+Zub1liW62XLbWXbbVettLe7CsbxdE7jDg6OqJBgwYAgJYtWyIqKgpffPEFli5dmu9YT0/PfD0gt2/fhqenZ5HnUKlUUKlU+fY7ODiY5EszVbnFGjgQGDAAivXr4fDSS8C//8qVWg0wbx4QGQlcvKjAlCkOWLky/zFma68ZsK3Wy5bay7ZaL2tvb0nbZvA6IxqNRmd8R24hISHYsWOHzr7IyMhCx5jYpC+/BCpXBqKj5XQYA1WoIG8QrFTK5Ux++83wKhIREZmSXmFk2rRp2Lt3L65cuYKYmBhMmzYNu3fvxrD/xjuMGDEC06ZN0x4/YcIEbN26FZ9++inOnj2LGTNm4PDhwxhvhBvGWQ1PT2D+fPl4+nTg/HmDi2zfHpg8WT4eOxa4e9fgIomIiExGrzCSkJCAESNGoGHDhujWrRuioqLw119/oUePHgCAa9euIS4uTnt8+/btsXr1aixbtgzBwcFYv349Nm3ahCZNmhi3FZZuxAigVy8gPR0YMwbQ5J8Jo6+ICCAwEEhIAF55RU77JSIiKo/0GqCwfPnyIl/fvXt3vn0DBw7EwIED9aqUzVEogKVLZXrYtw/46ivg1VcNKtLJSV6uadcOWL8e+PlnoH9/I9WXiIjIiHhvmvLC1xeYM0c+njoVuHbN4CJbtpT35wNktsnVaUVERFRuMIyUJ6++CnToIJdQffllo1xbefttGUoePABeecWOl2uIiKjcYRgpT5RKeWdflQrYsgX48UeDi3RwkJdrVCpg82Yltm+vbYSKEhERGQ/DSHnTsKGcVQMAEyYARaxUW1KBgcCsWfLx8uVBuHLF4CKJiIiMhmGkPJo8Wa7Gev8+8PrrRinyjTeA9u01SEuzx7hxdsaYsENERGQUDCPlkYMDsGIFYGcHrF0LFHNjwZKwswO++SYLKlUmdu9WYtEiw6tJRERkDAwj5VXz5sCbb8rHr74KPHxocJENGgAjR54CICfsGGF9NSIiIoMxjJRn778P+PvLObnZS6oaqHfvK+jWTYPHj4GRI4GsLKMUS0REVGoMI+WZk5OcXaNQyJ957vNTGkolsGxZFtzdgUOHgE8+MUI9iYiIDMAwUt498QQQHi4fjx0LpKQYXKSPD7BggXz8/vtATIzBRRIREZUaw4gl+OgjoHZtIDYWeO89oxQ5YgTQrx+QkSEfZ2QYpVgiIiK9MYxYAjc3ee8aAPj8c3l9xUDZt8OpWhU4dixnHRIiIqKyxjBiKXr3ll0YQsg7+6anG1ykpyewZIl8/NFHQFSUwUUSERHpjWHEksyfD9SoAZw+LdODEQwcCDz3nJxVM3Ik8PixUYolIiIqMYYRS1KlCrSrlX30EXDihFGKXbhQ9pKcOWO0ISlEREQlxjBiafr3B/7v/4DMTHm5JjPT4CKrVgW++UY+/uwzYO9eg4skIiIqMYYRS6NQyN6RSpWAw4flpRsj6NNHZhshgFGjgORkoxRLRERULIYRS+TlJbswAODdd412ueazz3JmEE+ZYpQiiYiIisUwYqlGjcpZKGTYMKOMPHV3B1aulI+/+gr46y+DiyQiIioWw4ilUijkQA8PD+DkSeCtt4xSbNeuwGuvycdjxgAPHhilWCIiokIxjFiy6tVzujIWLDBaV8acOYCfH3DzJjB0KKBWG6VYIiKiAjGMWLqnnsrpyhg1Crhzx+AiXVyAn36SP7duBV5+WQ5sJSIiMgWGEWswdy4QGAjEx8ub6RkhObRsCfz8s7zL74oVwMyZRqgnERFRARhGrIGzM/Djj4CjI/Drr8DXXxul2KefBhYvlo+nT8+5IkRERGRMDCPWIjg4Z4n4N94Azp83SrEvvQRMmyYfjxsHbNtmlGKJiIi0GEasyRtvAN26AampcrqvkUaefvihLC4zUy4Ae+yYUYolIiICwDBiXZRK4NtvgcqV5eqsM2YYpViFQo4b6dpVrswaGgpcu2aUoomIiBhGrE6tWjljRmbPNtqNZhwdgQ0bgCZNgLg4OYmHa5AQEZExMIxYo/79gRdekLNqnn8eePjQKMVWrAhs3izzzunT8n596elGKZqIiGwYw4i1+uILoH59eT0lPNxoxfr4yEDi7g7s2SOXNtFojFY8ERHZIIYRa+XmBvzwA2BnB6xeLaf+GknTpvKSjb09sGaN0VaiJyIiG8UwYs3atQPef18+fvVV4MoVoxXdrZsc1AoA8+YBixYZrWgiIrIxDCPW7u23gfbtgaQk2I0eDWRlGa3o558HZs2Sj197Ddi0yWhFExGRDWEYsXb29sD33wNublDu2we/DRuMWvzbb8vF0IQAhgwBDh0yavFERGQDGEZsQb16wJdfAgAarVkDxeHDRitaoZCXaPr0AdLSgL59gQsXjFY8ERHZAIYRWzFiBDT9+0OZlQW7ESPk6mVGkj2QtWVL4O5duQZJQoLRiiciIivHMGIrFApkLVqEx1WrQnHxIjBpklGLd3UF/vwTqFsXuHRJ9pCkphr1FEREZKUYRmxJlSo4OnEihEIhV2k18ohTDw9gyxagShXg33/lGBIjjpclIiIrxTBiY+4GBUHzxhvyyYsvyrXdjahhQ+C33wCVSv58/XU5uJWIiKgwDCM2SBMRATRrBty7Z5IlVDt0kGusKRTA4sVyHRIiIqLCMIzYIpVKrsrq5ARs26adaWNM/fsDn30mH0+dKk9HRERUEIYRWxUQAHz6qXw8dSoQE2P0U0ycCGRfERo1Cti1y+inICIiK8AwYsteeUUuEJKeDgwbJhcKMbJPPgEGDADUanmX35MnjX4KIiKycHqFkdmzZ6N169Zwc3NDjRo1EBYWhnPnzhX5nlWrVkGhUOhsTk5OBlWajEShAJYvB6pXlz0j06YZ/RRKpVwAtkMHIDERCA0Fbt0y+mmIiMiC6RVG9uzZg/DwcBw6dAiRkZFQq9Xo2bMnUlJSinyfu7s74uLitNvVq1cNqjQZkYdHzh3vPv9cjiExMicn4Ndf5Uyb69dlIElKMvppiIjIQtnrc/DWrVt1nq9atQo1atTAkSNH0LFjx0Lfp1Ao4OnpWboakuk9/bS8ZLNkiRzcceIEUK2aUU9RtapcgyQkBDh+XF66+fNPwMHBqKchIiILpFcYySsxMREAUKVKlSKPS05Ohq+vLzQaDVq0aIGPPvoIgYGBhR6fnp6O9PR07fOk//43Wq1WQ61WG1JlHdllGbPM8qzI9s6eDfudO6E4dw6aF19E1tq18jKOEXl7A5s2KdC9ux0iIxUYM0aDr7/OgtIEI5ds6bu1pbYCttVettV62Up7S9o+hRClW5JKo9GgX79+ePjwIfbt21focQcPHsSFCxfQtGlTJCYm4pNPPsHevXtx6tQpeHt7F/ieGTNmICIiIt/+1atXw8XFpTTVpRKoePkyOr75JpSZmYgOD8e1Hj1Mcp7Dh2vgo4/aQqNRokOHm3j99aNQqYy71gkREZlfamoqhg4disTERLi7uxd6XKnDyCuvvIItW7Zg3759hYaKgqjVagQEBGDIkCGYOXNmgccU1DPi4+ODu3fvFtkYfanVakRGRqJHjx5wsIHrBSVpr/KTT2D39tsQLi7IjIoC/PxMUpefflLgxRftoFYr0K6dBr/8koXq1Y1Xvi19t7bUVsC22su2Wi9baW9SUhKqVatWbBgp1WWa8ePH448//sDevXv1CiIA4ODggObNm+PixYuFHqNSqaBSqQp8rym+NFOVW14V2d433wS2bYNi9244jBoF7N9vkoEdI0YAtWvL6b6HDinx5JNKbN4sB7kaky19t7bUVsC22su2Wi9rb29J26bX1XohBMaPH4+NGzdi586dqFu3rt4Vy8rKQkxMDLy8vPR+L5UBOzvgu++ASpWAqChg3DiT3e2uc2fg4EF5p9/Ll+Xg1r17TXIqIiIqx/QKI+Hh4fjhhx+wevVquLm5IT4+HvHx8Xj8+LH2mBEjRmBarvUqPvjgA2zbtg2XL1/G0aNHMXz4cFy9ehUvvvii8VpBxuXjA6xcKRcJWbUKeOEFkwWSRo2AQ4eAdu2ABw+AHj3kfW2IiMh26BVGlixZgsTERHTu3BleXl7a7eeff9Yec+3aNcTluhPsgwcPMHbsWAQEBCA0NBRJSUk4cOAAGjdubLxWkPGFhQE//SR7Sr7/Hnj+eSAz0ySnqlED2LlT3s8mIwMYPhz44APe7ZeIyFboNWakJGNdd+/erfN8/vz5mD9/vl6VonJi0CAZRp57TgaTzEzZbWGC65vOzsDatcBbb8m7/E6fLi/dLFsGODoa/XRERFSO8N40VLT+/YH162UAWbcOGDxYdl+YgFIJfPwx8NVXMgN9+y3Qq5e8fENERNaLYYSK98wzwMaNsoti40Zg4EB5cz0Teekl4I8/ADc3YPduoH172UtCRETWiWGESqZPH3mDGZUK+O034NlnTXKX32y9ewP79slVW8+elQNcDx0y2emIiMiMGEao5Hr3ll0Wzs7A5s2yxyTXTCpja9oU+OcfoHlz4M4doEsXecWIiIisC8MI6ad7dxlEXFzkHX779gVSU012upo15dojTz8tO2IGDpQDXDnThojIejCMkP46dwa2bgUqVAB27JCXcJKTTXY6V1dg0yZg/Hj5/M035U2GTTTTmIiIyhjDCJXOk0/KnpHsUaZPPQU8emSy09nZAQsWAPPny5sJL10qO2X+u6EzERFZMIYRKr327YHISKBiRTnatFcvk6YDhQKYOBHYsEEOW9m6VWai69dNdkoiIioDDCNkmLZtge3b5b1sDh6U67k/fGjSU4aFyXEkHh7AiRNypk10tElPSUREJsQwQoZr1Uqu516lCvDvv3KQ6/37Jj/lP/8AgYHArVuyh+SPP0x6SiIiMhGGETKO5s1lIKlWDThyBOjWDbh3z6Sn9PUF9u+X2SclRc40XrjQpKckIiITYBgh4wkOBnbtkne+O3ZMLgxy545JT1mxopxpPGYMoNEAr70GvPGGyW4yTEREJsAwQsbVpImcXePpCcTEyEBy+7ZJT+ngAHz9NfDRR/L5558DgwbZIS3NzqTnJSIi42AYIeMLCJCBpGZN4NQpuS5JXJxJT6lQANOmyZsLq1TA778r8dZbTyImxqSnJSIiI2AYIdNo2BDYswfw8ZE3l+nUCbh50+Snfe45uQ5btWoCV65URLt29pg9mwukERGVZwwjZDoNGshA4usLXLggA8m1ayY/bYcOwJEjmWjdOg5qtQJvvy33nTlj8lMTEVEpMIyQadWtKwNJ3brApUsykFy5YvLTenkBb7/9L775JhMVK8oZx82bA59+ysGtRETlDcMImZ6vrwwkDRrIINKpE3D5sslPq1AAI0YInDwpF4dNTwcmT5anv3DB5KcnIqISYhihsuHjIwe1+vvLSzVlmAi8vYEtW4Bly+RN9/bvl7OQv/xSTgcmIiLzYhihslOrlgwkAQHAjRtAx47AunWAECY/tUIBjB0LnDwJdO0KPH4MvP66XJstNtbkpycioiIwjFDZ8vKSC6M1aQLExwODBslQcvhwmZze11fe22/RIsDFRWajpk3lXYDLIBMREVEBGEao7Hl4AIcOATNmyNvv7tsHtG4NjBxZJtN/lUrg1VflTfaefBJITgZeflmOK+EdgImIyh7DCJlHhQrA9OnA+fPA88/Lfd99J8eUzJwJpKaavAr168uekfnzAScn2WPSpAmwciV7SYiIyhLDCJmXt7cMIf/8A4SEyBDy/vty0bTVq02eCpRKYOJEeSuddu2ApCRg9GigXz95N2AiIjI9hhEqH9q0kdNc1qwBateWA1yHDQPat5eXdEysYUN5tWjuXMDREfjjD9lL8uOP7CUhIjI1hhEqPxQKYPBguXz8rFnyUs6hQ7LHZNgwkw/osLMD3nwTOHoUaNkSePAAGD4c6N/f5Pf6IyKyaQwjVP44OwPvvCPXIXnhBRlSVq+W3Rfvvy9HnJpQYCBw8KAcuuLgAGzcKHtJ1q0z6WmJiGwWwwiVX15ewIoVctpvx45ycZCZM2Uo+fZbk65Y5uAAvPsuEBUlp/7evStnIT/3nHxMRETGwzBC5V+LFnLayy+/yHvc3LoFjBoFtG0rB3qYUHCwDCTvvScv4/z8s+wl+fVXk56WiMimMIyQZVAogGeflbfe/fhjwM1N9pg8+aTssjDhMqqOjsAHH8jhK40by/EjYWHA//2fXNGViIgMwzBClkWlAqZMkeNJxo2Tc3PXrZNLzL/9NvDokclO3aoVcOQIMHWqPO2mTfISztChcrkUIiIqHYYRskweHnIN9+hoeYOZ9HRg9mzAzw9YvhzIyjLJaZ2cgDlzgJgYYOBAOe33p59kj8mYMfKmxEREpB+GEbJsTZvKpVN/+00Gkdu3gRdfhH27dqgeHW2yRUIaNwbWrpXTgJ9+WmafFSvkArLh4VwwjYhIHwwjZPkUCqBvXzmAY/58oFIlKI4fR/uICNg3bw58/bXJlpdv3hz4/Xc5Fbh7d0CtBhYvlkvNT54M3LljktMSEVkVhhGyHo6Ocm33CxeQ9frryHRyguL0aTm2xMdHjim5ccMkp27XTnbQ7NolF41NSwM+/RSoV0/OxHn40CSnJSKyCgwjZH2qVYPmk0/w1/LlyJo3D6hTB7h/X44pqVNHLhZioiXmO3eWs403b5YzkpOT5WKydesCH31k8vXaiIgsEsMIWa3MChWgmTABuHhRLqPaubMc3PHzz3KJ+bZt5cquGRlGPa9CATz1lJx5vGGDXNH14UO5qGy9evJK0uPHRj0lEZFFYxgh62dnJxcG2bVLzr554QU5Rfjff+U9b+rUAT780OgDPBQKuRbJ8ePyhnsNGshTTJokHy9ZYvQcRERkkRhGyLY0ayanvVy7Jlcy8/QE4uLk2u8+PsCLL8p5u0ZkZyfXIjl9GvjmG3lT4lu3gFdflSvbr1oFZGYa9ZRERBaFYYRsU40acmTp1avADz/IFc3S0+UaJU2bAl27yjXfjbheiYODXIvk/Hlg4UKZg65ckR01gYHAmjUmvd0OEVG5xTBCts3RUV6q+fdfYP9+ubS8nZ28pBMWJhcO+fxzICnJaKdUqeRaJJcuAfPmAVWryoAyZIjsuPn1V5Mtj0JEVC4xjBABcoBH+/ZycGtsrFzzvXJl4PJl4I03AG9vIHswrJG4uMi1SGJj5RUjd3d5hSgsTI6tXb+el2+IyDYwjBDl5eMj13y/cUMuOd+4sbznzYIFsqekb19gxw6jdV+4uckrRrGxwLRpMqRERcnl5uvXl/cFvH/fKKciIiqX9Aojs2fPRuvWreHm5oYaNWogLCwM586dK/Z969atQ6NGjeDk5ISgoCBs3ry51BUmKjMuLnLBtJMngW3bgD59ZAD54w+53GrNmsCIEXKqTEKCwaerUkWuRRIbK8fTVqsmx9lOnSrz0SuvyJsWExFZG73CyJ49exAeHo5Dhw4hMjISarUaPXv2REpKSqHvOXDgAIYMGYIxY8YgOjoaYWFhCAsLw0nee50shUIB9OghQ8i5c8D48YCrKxAfD3z/PTB8uLxxX4sWwFtvyfEm6emlPl2NGsDMmcD163LiT3CwXM3+q69kJ02vXsCWLRzsSkTWQ68wsnXrVowaNQqBgYEIDg7GqlWrcO3aNRw5cqTQ93zxxRfo3bs3pkyZgoCAAMycORMtWrTAwoULDa48UZnz9we+/BK4exfYuVN2WzRvLl+LjgbmzpUzcapUkT0pCxYAZ8+W6pKOk5OcaRMdnTOeVqGQnTShoUBAALBoEVd1JSLLZ2/ImxMTEwEAVapUKfSYgwcPYtKkSTr7evXqhU2bNhX6nvT0dKTn+j/LpP9mMqjVaqjVagNqrCu7LGOWWZ7ZUntN3lalEnjiCbnNnAncvg3Fjh1QRkZCsX07FLdvyzXh/7skKWrXhujeHZru3SG6dZODY/XQoYPcLl8GlixRYuVKJc6fV2D8eOCdd+zRqVMg/Pwy4ednisaWL/x3bJ1sqa2A7bS3pO1TCFG6UXgajQb9+vXDw4cPsW/fvkKPc3R0xLfffoshQ4Zo9y1evBgRERG4fft2ge+ZMWMGIiIi8u1fvXo1XFxcSlNdorIjBNyvXkWN6GhUP3YMVU+fhl2uX0ihVOJBgwZIaN4cd5o1wwN/fwg7O71O8fixPXbu9MGff9bDrVuuAAClUqBNmzg8/fRlBAbeg0Jh1FYREektNTUVQ4cORWJiItzd3Qs9rtRh5JVXXsGWLVuwb98+eHt7F3pcacJIQT0jPj4+uHv3bpGN0ZdarUZkZCR69OgBBwcHo5VbXtlSe8tVW1NTofj7byi2b4dy2zYo8oxCFe7uEF26QPToAU2PHvKueiWk0QB//pmFDz5IwvHjNbT7mzYVeP31LAwaJODkZLSWlAvl6rs1MbbVetlKe5OSklCtWrViw0ipLtOMHz8ef/zxB/bu3VtkEAEAT0/PfKHj9u3b8PT0LPQ9KpUKKpUq334HBweTfGmmKre8sqX2lou2VqwIPP203AA5ZXjbNrlFRkJx/z4Uv/4K/Por7AB545peveSg2S5d5AIkRejbF7CzO4g6dUKxZIkDvvsOOHFCgRdftMe0acDLL8uZOF5eJm9pmSoX320ZYVutl7W3t6Rt02sAqxAC48ePx8aNG7Fz507ULcH/wYWEhGDHjh06+yIjIxESEqLPqYmsh7c3MHq0XP89IUGu/jprFvDkk4C9vVxYbdEiOWK1ShU5WCQiAjhwoMhV0Bo3ljNubtyQy6R4e8sb882cCfj6ykk/UVFl10wiopLSK4yEh4fjhx9+wOrVq+Hm5ob4+HjEx8fjca77oY8YMQLTpk3TPp8wYQK2bt2KTz/9FGfPnsWMGTNw+PBhjB8/3nitILJUdnZA69bAO+8Ae/cC9+7J9eBffRXw85P3xjlwAJgxQ4aSqlXlrYAXLwYuXChwlk6VKnKST2wssHatXFhWrZbLobRpI4tZswZISyv75hIRFUSvMLJkyRIkJiaic+fO8PLy0m4///yz9phr164hLi5O+7x9+/ZYvXo1li1bhuDgYKxfvx6bNm1CkyZNjNcKImvh7g706yd7Rs6fl4li2TK5HGuVKvIeOZs2yZvb+PsDdevC7pVXUHP//nzLtNrby7ft3y97RIYPlzfrO3BA3gfH01Ou6bZvH++FQ0TmpdeYkZKMdd29e3e+fQMHDsTAgQP1ORURAUCdOsDYsXLLypKLjvw31gT79wNXr0K5fDlaAxCffCLvPtyjh9zat5c3AoTc/f33cmn5r74CVq6Ui6p9/bXc6tUDnn9ebvXrm7XFRGSDeG8aIkthZydTxdtvy1XQHjwA/vwTWa+/jqTataEQQnaBfPSRHPiavfDa558Dp04BQsDLSw4/uXJFrtk2apRcTPbyZbm/QQO5dMrSpbJ4IqKywDBCZKkqVABCQ6H55BPsWrAA6thYYNUqYNgwuaZ8SopcdO2NN4AmTeSI1lGjgB9/hDL+Frp0Fli5Uq5q/8MPcgKPUik7XF5+WV7GGTgQ+P13OeaEiMhUGEaIrEWtWsDIkTJZxMUBx44B8+YBPXvKteVv3QK+/VYOHqlVC6heHejcGRXeeg3DUpZh6/sHcON0EubNk9klIwNYv14OYalVC5gwAThyhONLiMj4DFoOnojKKaVS3mEvOBiYPFlOndm3T4412bYNOHFCztzZs0du//ECMLl2bfwvKAgJrYMQGdcEy/8Nwv47jbBggSMWLJBTiEeMkB0wxSwzRERUIgwjRLbAyQno3l1uc+cCjx8DZ84AMTHAyZPyZ0yM7D25dg2Ka9fggT8xHMBwABo7e9yq4I+DyUE4djoIB95qgmVvBaFe1zp4fqQSzz4rx54QEZUGwwiRLXJ2Blq0kFtu9+/LcJI7oJw8CWViIryTTmMgTmMgcqbyJ++sgFM7A7F+dBAcmjdBo4FBaPZ8EOy8aoCIqKQYRogoR5UqQMeOcssmhFzWNXcvysmTEKdPwzUjBW3xL9pm/QschtymAilOVZHpHwC3NgFQNg4AGjUCAgKA2rXlJSQiolwYRoioaAoF4OMjt9DQnN2ZmcCFCxAxJ3Fzawzu7j4Jt6sxqKu5hApp94AT++SWm7Mz0LChDCYBuUKKnx9QwP2oiMg2MIwQUenY2wMBAVAEBMB70EB4A0hPB/7YmIp/vjuH27vPwPfxGTTCWQTgDPxxHo6PH8tZPseO6ZalVMqV1/KGlIAAeaNBIrJqDCNEZDQqFdDvORf0e645MjKaY/duYONG4PVNwJ34TNTDZTTCWQTZnUFXrzMIcjyLanfOQPkoSd4g8OJFubBJbl5eOuFE0aABnBMS5Iq0Vny3UyJbwjBCRCbh6CiXOOnZU95q59Ahe2zc6I+NG/3x+6V++OiGPE6pEPi/kDgMbXEWXTzPoHL8GTnT5+xZObsnLk5uu3YBkP/R6glAjB8P1K0r16+vX18uH5v9s04dXvYhsiAMI0RkckqlvFVO+/by/jgnT8oek40bgWPHFPjlYE38crAmgK5o1UremPj/FgIBNRNlKDl7VgaUM2cgzpyBiI2FMiMDOHdObnllj3PJDii5w0r9+oCbW5l/BkRUOIYRIipTCgUQFCS399+XNybODib79wOHD8vtnXeAhg0r4tln2+L//q8tWo2Q781Uq7H5998RGhQEh2vXgEuX5OWdS5dyHqekANeuyW3nzvyVqFGj4B6V+vWBatXkiYiozDCMEJFZ1a0LTJokt9u3gd9+AzZsAHbskJ0es2fLzdsbCAsD+vZVIFPYy0sxfn5At266BQoBJCQUHFIuXQLu3pWvJyQABw/mr5C7uxyj0rhxziDagABZUTu7svhIiGwOwwgRlRseHsDYsXJLTJT3+du4Uf68cQNYuBBYuNAeTk6h6NZNiV695JgUf/9cnRkKhSzIw0NeF8orMTEnoOQNKjduAElJwL//yi03lUqeKCBAN6j4+3N8CpGBGEaIqFyqWBEYMkRujx8D27fLYPL77wJ379rjzz+BP/+Ux9auDfToIYNJt25A1arFFFzQ6rOAvIfPpUva8Sna7exZ+Vr2qrS55Z2WnB1WGjWSvSxEVCyGESIq95ydgb595ZaenolFi/YjLe1J7Nhhh3375NCQ5cvlplAALVvmhJP27eXMnhJxcgICA+WWW1YWcPVq/pBy5gzw8GHh05Jr1dINKQEBgKcnkJmpu6nVBT5XpKXB+/BhKO7eleUVcax2q1IF8PXN2Tw9ueotlXsMI0RkUZRKoH79RISGavD223ZITQX27pU3I46MlDN1sgfBzp4NVKgAdOokg0mPHjIP6D0+1c5O9n7Uqwf06ZOzXwggPr7gkBIXB9y8Kbft20vVVnsALUv1zlwcHeXMotwBJffm7a1HWiMyDYYRIrJoLi5A795yA+TSJNu354SThAQ55mTzZvl6rVo5vSbduwPVqxtwcoVCLsrm5QV07ar72sOH8vLO6dO6IeX+fblYm719zpb7ea7HGnt73H34ENU8PaF0dCz+PUolcOeO7MW5elUGoYyMnLExhbWhZs3Cw4qvr0x0RCbEMEJEVqVmTWDECLlpNHKIR3Yw2btX/n1etUpuANC8eU6vSYcO8kqNUVSqBLRrJ7dSylKrcXDzZoSGhkJZmtVmMzNlg7PDSd7t2jU5Fia7B+fAgYLLqVpVhpLateXjypVl+ypXztlyP69Uib0tpBeGESKyWkolEBwstylT5EDYfftywsnx40B0tNzmzpVjUzp2lJ0cXbvKoGLRs3nt7XN6NwqSPQ26sLBy9aqcfXTvntyOHi35uV1cCg8rBYUXV1eo7t+XAYrL/NschhEishnOzrIHpEcP+Tw+Xl7SiYyUASU+HvjrL7kBcuJNp05Aly4ynDRpYmVjQXNPg27TpuBjEhNzgsmNG/Iy04MHcnv4MOdx9vPERPm+1FS53bxZoqo4AOgNQIwZI6+deXnJwbeenjmP8+5zdeUCdVaCYYSIbJanJzB8uNyEAE6dkuFk1y5g9275d/W33+QGyCsUXbrkhJOGDW3gb2HFikDTpnIriaws+cEVFlYKeS7++6nQaHIWpTt+vOhzubiULLRUry57iUxNCDlGJzVVrgKcd8u1X5mUBP9jx6D8+295u+tijkd6umyvu7u8nYGbW+kfl8VnoafyVyMiIjNQKGTPR5MmwMSJ8m9qdLRcTX7nTnl55949YP16uQHy71zucFK3rg2Ek+LY2cnpxVWq6PU27TL/bdrA4e5d2U0VHy9nJeX+mf04OVn+sS5qcG42hUIGEjc3GRhMtT1+LP/hlORjAhCg1ycEuSBffLy+78rPyangkDJnjvwFMAOGESKiAtjZAa1aye3NN+X/8EZFyV6TnTvlWM+4OGD1arkBcnxn16454cTb27xtsDh2dvKSUUk+uORkef+AvCElb3C5fVuOZM7ubSkrDg5yFpKLi/yZe3NxgcbZGdfu3YNPQADs3NwKPU77WKWSPSSPHslQ8uhRzpb7eVGPMzJk3dLS5Jb383j//bL7fPJgGCEiKgFHRznbpkMH4N135X/LDx7MCSf//CMnp+SeqdOgQU446dJF/p0lI3F1lVv9+kUfl5Ul70cUFyd7LgDZU2KKzckpJzwUMwg3S63G8c2bUSs0FHZlNWA3I6Po0FLcZ2lCDCNERKXg5JQTMj74QP6P+v79OeHkyJGchVmXLZPvadwY6NwZCAmRW716vKxjctm9LUyCMlFXrVrM/RLMg2GEiMgIXF2BXr3kBsgxnHv35oST48fl+menTwOLF8tjqleXy5Bkh5PWrbm+GNkmhhEiIhOoWDHnfjqAvFKwZ4/sPTl4UC7ZceeOvJ1N9i1t7OzkpJXscNKypRwXSWTtGEaIiMpAtWpA//5yA+SYk+hoGUyyt5s3cxZhk70nDqhYsTeeeMIOHTqw94SsF8MIEZEZODnl9IBku3FDN5wcPSqQmKjCn38Cf/4pj8nuPcl9ead+fY49IcvGMEJEVE54ewMDB8oNAJKTM7Fo0UEolR0QFWWHgwdlYMnuPVmyRB6Xe+xJmzZAixZylXUiS8EwQkRUTqlUQKNGDxAaqoGDg7xJTv7ek/xjTwA5U6dlS92NAYXKK4YRIiILkrf3JD1dd+zJ4cNAbCxw+bLc1q3LeS8DCpVXDCNERBZMpZKXaNq1A954Q+67f1/2mBw5krNlh5O8AaVuXbnKbHY4adFC75XciQzGMEJEZGWqVAG6d5dbtgcPZEA5fFg3oMTGyi1vQMkOJ61aMaCQ6TGMEBHZgMqVgW7d5JYtO6Dk7kG5dCknoGTfEBDICSgtWuRs1auXfTvIOjGMEBHZKEMDire3bjhp0QKoWZPTjEl/DCNERKRVUEB5+DAnoBw9Krfz5+XMnhs3gN9+yzm2Ro38AaVOHQYUKhrDCBERFalSJXn34a5dc/YlJcn77WSHk+hoed+dhARg61a55X5/8+a6AcXPTy7gRgQwjBARUSm4uwNPPim3bI8fAzExOQHl6FH5/OFDecPAXbtyjq1QAWjWLCecBAUBmZnsPrFVDCNERGQUzs5yBdg2bXL2ZWTIHpPcAeXYMSAlRd40cP/+7CMdYG/fB40bK9G0KdCkiQwoQUFybAov81g3hhEiIjIZR0fZA9KsGTB6tNyXlQWcO6cbUKKjBZKS7HDiBHDihG4ZlSrlhJPcIaVSpbJtC5kOwwgREZUpOzugcWO5DR8u96WnZ2LVqt2oXr0LzpyxR0yMvMRz7py8zLNvn9xy8/bOCSbZQSUgQC4ER5ZF7zCyd+9ezJs3D0eOHEFcXBw2btyIsLCwQo/fvXs3unTpkm9/XFwcPD099T09ERFZIaUS8PRMRWiowLPP5uxPT5eBJDucZG/Xr+fM5tmyJed4OzvA3183oAQFyXVSlMqybxeVjN5hJCUlBcHBwRg9ejSezf0vphjnzp2Du7u79nmNGjX0PTUREdkYlQpo2lRuuSUmAidP5g8pDx8CZ87Ibe3anOMrVJC9Jv7+ciaPv3/O44oVy7RJVAC9w8hTTz2Fp556Su8T1ahRA5VKeIEvPT0d6enp2udJSUkAALVaDbVarfe5C5NdljHLLM9sqb1sq/WypfayrYVzcck/WFYI4NYt4ORJBU6eVODUKfnzzBkgJUWBw4flcvh5eXgI+PkJNGgA+PkJ7Va/PuDkZIzW5Wcr321J26cQQojSnkShUJT4Mo2vry/S09PRpEkTzJgxAx06dCj0PTNmzEBERES+/atXr4aLi0tpq0tERDYoK0uBuLgKuHnTFbduueLWrQr//XTFgweFpw2FQqB69VR4eaWgVq1k1KyZgpo1k1GzZjKqV0/lOiklkJqaiqFDhyIxMVHn6kheJg8j586dw+7du9GqVSukp6fjm2++wffff49//vkHLVq0KPA9BfWM+Pj44O7du0U2Rl9qtRqRkZHo0aMHHBwcjFZueWVL7WVbrZcttZdtNb2kJODiReDCBUWuDTh/XoGkpMLnEzs6CtStK3tS/P1lT0q9ekC9egLe3sUv6GYr321SUhKqVatWbBgx+Wyahg0bomHDhtrn7du3x6VLlzB//nx8//33Bb5HpVJBVcBwaAcHB5N8aaYqt7yypfayrdbLltrLtppO1apya9tWd78QwJ07ctl7GU5ytgsXgPR0Bc6dA86dyx9YHBzkEvj168utXr2cn/XqyfErOcda93db0raZZWpvmzZtsC/vHC0iIqJyQqGQ99mpUQN44gnd1zQaOZsnd1A5dw64fFneTFCtlvsvXCi4bE9PoG5dO6hULXDkiBJ+fjnBpUYN21zgzSxh5NixY/Dy8jLHqYmIiAyiVAK+vnLr0UP3tawsOd348mV5t+Psn9mPHzwA4uOB+HglAB/s3q37/goVcnpQ8vas1Kkje12skd5hJDk5GRcvXtQ+j42NxbFjx1ClShXUrl0b06ZNw82bN/Hdd98BAD7//HPUrVsXgYGBSEtLwzfffIOdO3di27ZtxmsFERFROWBnlxNUClhiCw8eyGBy/nwmtmw5DweHRrhyRYlLl2RvS0pKzjTlvOztZSjx9wcaNsz52bCh5feo6B1GDh8+rLOI2aRJkwAAI0eOxKpVqxAXF4dr165pX8/IyMD//vc/3Lx5Ey4uLmjatCm2b99e4EJoRERE1qxyZaBVKyA4WKBChQsIDfWDg4NcjS09Hbh6teAelUuX5I0I5TgV4Pffdct1d9cNKNk//fx0x6iUV3qHkc6dO6OoCTirVq3Sef7mm2/izTff1LtiREREtkSlylmMLS+NBrh5UwaR7DEq2Y+vXJGzgqKi5JaXt3fBQcXXt/hZP2WF96YhIiIq55RKwMdHbt27676WliZ7TgoKKvfu5Sybv2OH7vscHYEGDXLCyZgxsifFHBhGiIiILJiTExAYKLe87t3LH1DOnZNrq6SnA6dPyw0A+vVjGCEiIiIjq1oVCAmRW25ZWcC1a7pBJSDAPHUEGEaIiIhsjp2dvJNx3bpAr17mrg3AGyoTERGRWTGMEBERkVkxjBAREZFZMYwQERGRWTGMEBERkVkxjBAREZFZMYwQERGRWTGMEBERkVkxjBAREZFZMYwQERGRWTGMEBERkVkxjBAREZFZMYwQERGRWVnEXXuFEACApKQko5arVquRmpqKpKQkODg4GLXs8siW2su2Wi9bai/bar1spb3Zf7ez/44XxiLCyKNHjwAAPj4+Zq4JERER6evRo0eoWLFioa8rRHFxpRzQaDS4desW3NzcoFAojFZuUlISfHx8cP36dbi7uxut3PLKltrLtlovW2ov22q9bKW9Qgg8evQINWvWhFJZ+MgQi+gZUSqV8Pb2Nln57u7uVv2PIS9bai/bar1sqb1sq/WyhfYW1SOSjQNYiYiIyKwYRoiIiMisbDqMqFQqTJ8+HSqVytxVKRO21F621XrZUnvZVutla+0tjkUMYCUiIiLrZdM9I0RERGR+DCNERERkVgwjREREZFYMI0RERGRWDCNERERkVlYfRhYtWoQ6derAyckJbdu2xb///lvk8evWrUOjRo3g5OSEoKAgbN68uYxqapjZs2ejdevWcHNzQ40aNRAWFoZz584V+Z5Vq1ZBoVDobE5OTmVU49KbMWNGvno3atSoyPdY6vdap06dfG1VKBQIDw8v8HhL+0737t2Lvn37ombNmlAoFNi0aZPO60IIvP/++/Dy8oKzszO6d++OCxcuFFuuvr/3ZaGotqrVakydOhVBQUGoUKECatasiREjRuDWrVtFllma34WyUNz3OmrUqHz17t27d7HllsfvFSi+vQX9DisUCsybN6/QMsvrd2sqVh1Gfv75Z0yaNAnTp0/H0aNHERwcjF69eiEhIaHA4w8cOIAhQ4ZgzJgxiI6ORlhYGMLCwnDy5Mkyrrn+9uzZg/DwcBw6dAiRkZFQq9Xo2bMnUlJSinyfu7s74uLitNvVq1fLqMaGCQwM1Kn3vn37Cj3Wkr/XqKgonXZGRkYCAAYOHFjoeyzpO01JSUFwcDAWLVpU4Osff/wxFixYgK+++gr//PMPKlSogF69eiEtLa3QMvX9vS8rRbU1NTUVR48exXvvvYejR49iw4YNOHfuHPr161dsufr8LpSV4r5XAOjdu7dOvX/66aciyyyv3ytQfHtztzMuLg4rVqyAQqFA//79iyy3PH63JiOsWJs2bUR4eLj2eVZWlqhZs6aYPXt2gccPGjRI9OnTR2df27ZtxUsvvWTSeppCQkKCACD27NlT6DErV64UFStWLLtKGcn06dNFcHBwiY+3pu91woQJon79+kKj0RT4uqV+p0IIAUBs3LhR+1yj0QhPT08xb9487b6HDx8KlUolfvrpp0LL0ff33hzytrUg//77rwAgrl69Wugx+v4umENBbR05cqR45pln9CrHEr5XIUr23T7zzDOia9euRR5jCd+tMVltz0hGRgaOHDmC7t27a/cplUp0794dBw8eLPA9Bw8e1DkeAHr16lXo8eVZYmIiAKBKlSpFHpecnAxfX1/4+PjgmWeewalTp8qiega7cOECatasiXr16mHYsGG4du1aocday/eakZGBH374AaNHjy7y7tWW+p3mFRsbi/j4eJ3vrmLFimjbtm2h311pfu/Lq8TERCgUClSqVKnI4/T5XShPdu/ejRo1aqBhw4Z45ZVXcO/evUKPtabv9fbt2/jzzz8xZsyYYo+11O+2NKw2jNy9exdZWVnw8PDQ2e/h4YH4+PgC3xMfH6/X8eWVRqPBxIkT0aFDBzRp0qTQ4xo2bIgVK1bg119/xQ8//ACNRoP27dvjxo0bZVhb/bVt2xarVq3C1q1bsWTJEsTGxuLJJ5/Eo0ePCjzeWr7XTZs24eHDhxg1alShx1jqd1qQ7O9Hn++uNL/35VFaWhqmTp2KIUOGFHlHV31/F8qL3r1747vvvsOOHTswd+5c7NmzB0899RSysrIKPN5avlcA+Pbbb+Hm5oZnn322yOMs9bstLXtzV4CMLzw8HCdPniz2+mJISAhCQkK0z9u3b4+AgAAsXboUM2fONHU1S+2pp57SPm7atCnatm0LX19frF27tkT/t2Gpli9fjqeeego1a9Ys9BhL/U4ph1qtxqBBgyCEwJIlS4o81lJ/F5577jnt46CgIDRt2hT169fH7t270a1bNzPWzPRWrFiBYcOGFTuw3FK/29Ky2p6RatWqwc7ODrdv39bZf/v2bXh6ehb4Hk9PT72OL4/Gjx+PP/74A7t27YK3t7de73VwcEDz5s1x8eJFE9XONCpVqgR/f/9C620N3+vVq1exfft2vPjii3q9z1K/UwDa70ef7640v/flSXYQuXr1KiIjI4vsFSlIcb8L5VW9evVQrVq1Qutt6d9rtr///hvnzp3T+/cYsNzvtqSsNow4OjqiZcuW2LFjh3afRqPBjh07dP7PMbeQkBCd4wEgMjKy0OPLEyEExo8fj40bN2Lnzp2oW7eu3mVkZWUhJiYGXl5eJqih6SQnJ+PSpUuF1tuSv9dsK1euRI0aNdCnTx+93mep3ykA1K1bF56enjrfXVJSEv75559Cv7vS/N6XF9lB5MKFC9i+fTuqVq2qdxnF/S6UVzdu3MC9e/cKrbclf6+5LV++HC1btkRwcLDe77XU77bEzD2C1pTWrFkjVCqVWLVqlTh9+rQYN26cqFSpkoiPjxdCCPH888+Lt956S3v8/v37hb29vfjkk0/EmTNnxPTp04WDg4OIiYkxVxNK7JVXXhEVK1YUu3fvFnFxcdotNTVVe0ze9kZERIi//vpLXLp0SRw5ckQ899xzwsnJSZw6dcocTSix//3vf2L37t0iNjZW7N+/X3Tv3l1Uq1ZNJCQkCCGs63sVQs4aqF27tpg6dWq+1yz9O3306JGIjo4W0dHRAoD47LPPRHR0tHYGyZw5c0SlSpXEr7/+Kk6cOCGeeeYZUbduXfH48WNtGV27dhVffvml9nlxv/fmUlRbMzIyRL9+/YS3t7c4duyYzu9wenq6toy8bS3ud8Fcimrro0ePxOTJk8XBgwdFbGys2L59u2jRooXw8/MTaWlp2jIs5XsVovh/x0IIkZiYKFxcXMSSJUsKLMNSvltTseowIoQQX375pahdu7ZwdHQUbdq0EYcOHdK+1qlTJzFy5Eid49euXSv8/f2Fo6OjCAwMFH/++WcZ17h0ABS4rVy5UntM3vZOnDhR+9l4eHiI0NBQcfTo0bKvvJ4GDx4svLy8hKOjo6hVq5YYPHiwuHjxovZ1a/pehRDir7/+EgDEuXPn8r1m6d/prl27Cvx3m90mjUYj3nvvPeHh4SFUKpXo1q1bvs/B19dXTJ8+XWdfUb/35lJUW2NjYwv9Hd61a5e2jLxtLe53wVyKamtqaqro2bOnqF69unBwcBC+vr5i7Nix+UKFpXyvQhT/71gIIZYuXSqcnZ3Fw4cPCyzDUr5bU1EIIYRJu16IiIiIimC1Y0aIiIjIMjCMEBERkVkxjBAREZFZMYwQERGRWTGMEBERkVkxjBAREZFZMYwQERGRWTGMEBERkVkxjBAREZFZMYwQERGRWTGMEBERkVn9P7EDrTdZddZYAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, color=\"blue\", label=\"Train\")\n",
    "plt.plot(valid_losses, color=\"red\", label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Average loss per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4s/mv0mfx752sb76dmyd9_tr18w0000gn/T/ipykernel_46015/2036248109.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  translator.load_state_dict(torch.load(f\"models/translator_transformer_{num_layers}_layers_bestval.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TranslatorModel(\n",
       "  (encoder): Encoder(\n",
       "    (token_embedding): Embedding(5374, 256)\n",
       "    (positional_embedding): Embedding(100, 256)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (ff): PositionWiseFeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mha): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_embedding): Embedding(4556, 256)\n",
       "    (positional_embedding): Embedding(100, 256)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (fc): Linear(in_features=256, out_features=4556, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (ff): PositionWiseFeedForward(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (enc_attention): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.load_state_dict(torch.load(f\"models/translator_transformer_{num_layers}_layers_bestval.pt\", map_location=device))\n",
    "translator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: <BOS> drei personen gehen mitten am tag auf einem pfad . <EOS>\n",
      "TARGET: <BOS> three people walking on a trail in the middle of the day . <EOS>\n",
      "MODEL: three people walk on a path in the middle of a day . <EOS> .\n"
     ]
    }
   ],
   "source": [
    "random_eval_idx = int(np.random.choice(list(range(NUM_EXAMPLES))))\n",
    "print_sentences(data=train_data, idx=random_eval_idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: <BOS> ein mann geht an einem silbernen fahrzeug vorbei . <EOS>\n",
      "TARGET: <BOS> a man walks by a silver vehicle . <EOS>\n",
      "MODEL: a man walks past a silver vehicle . <EOS> .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SOURCE: <BOS> drei personen mit langen hosen sitzen im freien auf kleinen steinen hinter einem großen busch . <EOS>\n",
      "TARGET: <BOS> three people sitting outside on small rocks behind a big bush , wearing long pants . <EOS>\n",
      "MODEL: three people wearing outside on large stone behind a large bush wearing wearing long pants . <EOS> .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SOURCE: <BOS> eine <UNK> mit fünf mädchen , die nacheinander springen . <EOS>\n",
      "TARGET: <BOS> a ballet class of five girls jumping in <UNK> . <EOS>\n",
      "MODEL: a ballet dancer with five girls , in martial . <EOS> .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SOURCE: <BOS> ein schickes mädchen spricht mit dem handy während sie langsam die straße <UNK> . <EOS>\n",
      "TARGET: <BOS> a <UNK> girl talking on her cellphone while gliding slowly down the street . <EOS>\n",
      "MODEL: a fashionable girl talks on her cellphone while she slowly down the street . <EOS> .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SOURCE: <BOS> zwei männer mitten in einem kampf . <EOS>\n",
      "TARGET: <BOS> two men in the middle of the action of a fighting match . <EOS>\n",
      "MODEL: two men in the middle of a ring match a match match . <EOS> <EOS>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idxs = [42, 422, 10, 7, 999]\n",
    "\n",
    "for idx in idxs:\n",
    "    print_sentences(data=train_data, idx=idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def get_tokenizer_fn(nlp: Language, lower: bool):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn\n",
    "\n",
    "tokenizer_fn = get_tokenizer_fn(en_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:23<00:00, 42.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.3579121631025168,\n",
       " 'precisions': [0.67200546531046,\n",
       "  0.4322326269098078,\n",
       "  0.2882584571326293,\n",
       "  0.19598977786514646],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0088834431000153,\n",
       " 'translation_length': 13174,\n",
       " 'reference_length': 13058}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute BLEU metric on test data\n",
    "predictions, references = [], []\n",
    "for idx in tqdm(range(test_data.num_rows)):\n",
    "    data_eval_src = test_data[idx][\"de_ids\"].reshape(1, -1).to(device)\n",
    "    sentence_evaluated = translate(model=translator,\n",
    "                                   src=data_eval_src,\n",
    "                                   src_lang=src_lang,\n",
    "                                   tgt_lang=tgt_lang,\n",
    "                                   max_tgt_length=100,\n",
    "                                   device=device,\n",
    "                                   clean=True)\n",
    "\n",
    "\n",
    "    predictions.append(sentence_evaluated)\n",
    "    references.append(test_data[idx][\"en\"])\n",
    "\n",
    "bleu.compute(predictions=predictions, references=references, tokenizer=tokenizer_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
