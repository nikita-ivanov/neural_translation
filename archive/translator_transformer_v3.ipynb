{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nivanov/miniconda3/envs/dlmetal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections.abc import Callable\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "from spacy.language import Language\n",
    "from tokenizers import Tokenizer, decoders, normalizers\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from torch import Tensor, nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # for Apple chips\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': ['Two young, White males are outside near many bushes.',\n",
       "  'Several men in hard hats are operating a giant pulley system.'],\n",
       " 'de': ['Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       "  'Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craete_tokenizer(max_vocab_size: int = 4000) -> tuple[Tokenizer, WordPieceTrainer]:\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"<UNK>\"))  # noqa: S106\n",
    "    tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"<BOS> $A <EOS>\",\n",
    "        special_tokens=[\n",
    "            (\"<BOS>\", 1),\n",
    "            (\"<EOS>\", 2),\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.decoder = decoders.WordPiece()\n",
    "    trainer = WordPieceTrainer(vocab_size=max_vocab_size,\n",
    "                               special_tokens=[\"<UNK>\", \"<BOS>\", \"<EOS>\", \"<PAD>\"])\n",
    "\n",
    "    return tokenizer, trainer\n",
    "\n",
    "def batch_iterator(batch_size: int, data: datasets.Dataset, key: str) -> list[str]:\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i : i + batch_size][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_lang, src_lang_trainer = craete_tokenizer()\n",
    "tgt_lang, tgt_lang_trainer = craete_tokenizer()\n",
    "\n",
    "src_lang.train_from_iterator(batch_iterator(100, train_data, \"de\"), src_lang_trainer)\n",
    "tgt_lang.train_from_iterator(batch_iterator(100, train_data, \"en\"), tgt_lang_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29000/29000 [00:02<00:00, 11874.73 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:00<00:00, 11207.77 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 11190.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_example(example: dict[str, str],\n",
    "                     en_tokenizer: Tokenizer,\n",
    "                     de_tokenizer: Tokenizer,\n",
    "                     max_length: int) -> dict[str, list[str | int]]:\n",
    "    en_encoded = en_tokenizer.encode(example[\"en\"])\n",
    "    de_encoded = de_tokenizer.encode(example[\"de\"])\n",
    "\n",
    "    return {\"en_tokens\": en_encoded.tokens[:max_length],\n",
    "            \"de_tokens\": de_encoded.tokens[:max_length],\n",
    "            \"en_ids\": en_encoded.ids[:max_length],\n",
    "            \"de_ids\": de_encoded.ids[:max_length]}\n",
    "\n",
    "max_length = 1000\n",
    "sos_token = \"<BOS>\"  # noqa: S105\n",
    "eos_token = \"<EOS>\"  # noqa: S105\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_tokenizer\": tgt_lang,\n",
    "    \"de_tokenizer\": src_lang,\n",
    "    \"max_length\": max_length,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size src: 4000\n",
      "Vocab size tgt: 4000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size src: {src_lang.get_vocab_size()}\")\n",
    "print(f\"Vocab size tgt: {tgt_lang.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ein kleines madchen klettert in ein spielhaus aus holz.\n",
      "a little girl climbing into a wooden playhouse.\n"
     ]
    }
   ],
   "source": [
    "print(src_lang.decode(train_data[2][\"de_ids\"]))\n",
    "print(tgt_lang.decode(train_data[2][\"en_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    "    )\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index: int) -> Callable[[dict], dict]:\n",
    "    def collate_fn(batch: dict) -> dict[str, int]:\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        return {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset: datasets.Dataset, batch_size: int, pad_index: int, *,\n",
    "                    shuffle: bool = True, pin_memory: bool = False) -> torch.utils.data.DataLoader:\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, tgt_lang.token_to_id(\"<PAD>\"))\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, tgt_lang.token_to_id(\"<PAD>\"))\n",
    "test_data_loader = get_data_loader(test_data, batch_size, tgt_lang.token_to_id(\"<PAD>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, dropout_ratio: float = 0.1):\n",
    "        if hidden_dim % num_heads != 0:\n",
    "            msg = \"hidden_dim must be divisible by num_heads\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scaling = 1 / (self.head_dim ** .5)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            q: query of size (batch_size, seq_length, hidden_dim)\n",
    "            k: query of size (batch_size, seq_length, hidden_dim)\n",
    "            v: query of size (batch_size, seq_length, hidden_dim)\n",
    "            mask: optional mask of size (batch_size, 1, 1, seq_length)\n",
    "                  or (batch_size, 1, seq_length, seq_length)\n",
    "        Outputs\n",
    "            attention weighted embedding vectors of size (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # all Q, K, V are of shape (batch_size, seq_length, hidden_dim)\n",
    "        Q = self.fc_q(q)\n",
    "        K = self.fc_k(k)\n",
    "        V = self.fc_v(v)\n",
    "\n",
    "        batch_size, seq_length, _ = Q.size()\n",
    "\n",
    "        # all Q, K, V are of shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # energy.shape (batch_size, num_heads, seq_length, seq_length)\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) * self.scaling  # type: Tensor\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -torch.inf)\n",
    "\n",
    "        # attention.shape (batch_size, num_heads, seq_length, seq_length)\n",
    "        attention = energy.softmax(dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # x.shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, num_heads, head_dim)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, hidden_dim)\n",
    "        x = x.reshape(batch_size, seq_length, self.hidden_dim)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, hidden_dim)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, ff_dim: int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.fc_2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        # x.shape (batch_size, seq_length, emb_dim)\n",
    "        x = self.fc_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, ff_dim:int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.ff = PositionWiseFeedForward(hidden_dim=hidden_dim, ff_dim=ff_dim)\n",
    "\n",
    "        self.mha = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                      num_heads=num_heads,\n",
    "                                      dropout_ratio=dropout_ratio)\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            input of size (batch_size, seq_length, hidden_dim)\n",
    "            mask of size (batch_size, 1, 1, seq_length)\n",
    "        Outputs\n",
    "            (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # x1.shape (batch_size, seq_length, hidden_dim)\n",
    "        x1 = self.mha(src, src, src, mask=mask)  # type: Tensor\n",
    "        x1 = self.norm_1(self.dropout(x1) + src)\n",
    "\n",
    "        # x2.shape (batch_size, seq_length, hidden_dim)\n",
    "        x2 = self.ff(x1)\n",
    "        x2 = self.norm_2(x1 + self.dropout(x2))\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int,\n",
    "                 num_heads: int, num_layers: int, ff_dim: int, max_seq_length: int,\n",
    "                 device: torch.device, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.scaling = hidden_dim ** (0.5)\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim=hidden_dim,\n",
    "                                                  num_heads=num_heads, ff_dim=ff_dim,\n",
    "                                                  dropout_ratio=dropout_ratio)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            input of shape (batch_size, seq_legth)\n",
    "        Outputs\n",
    "            encoded sequence of shape (batch_size, seq_legth, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = src.size()\n",
    "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "        x = self.token_embedding(src) * self.scaling + self.positional_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int,\n",
    "                 ff_dim:int, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_3 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.num_heads = num_heads\n",
    "        self.ff = PositionWiseFeedForward(hidden_dim=hidden_dim, ff_dim=ff_dim)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                                 num_heads=num_heads,\n",
    "                                                 dropout_ratio=dropout_ratio)\n",
    "\n",
    "        self.enc_attention = MultiHeadAttention(hidden_dim=hidden_dim,\n",
    "                                                num_heads=num_heads,\n",
    "                                                dropout_ratio=dropout_ratio)\n",
    "\n",
    "    def forward(self, dec_input: Tensor, enc_outputs: Tensor,\n",
    "                enc_mask: Tensor, dec_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_input of shape (batch_size, seq_length, hidden_dim)\n",
    "            enc_input of shape (batch_size, seq_length, hidden_dim)\n",
    "            enc_mask of shape (batch_size, 1, 1, seq_length)\n",
    "            dec_mask of shape (batch_size, 1, seq_length, seq_length)\n",
    "        Outputs\n",
    "            (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        # x1.shape (batch_size, seq_length, hidden_dim)\n",
    "        x1 = self.self_attention(dec_input, dec_input, dec_input, mask=dec_mask)\n",
    "\n",
    "        x1 = self.norm_1(dec_input + self.dropout(x1))\n",
    "\n",
    "        # x2.shape (batch_size, seq_length, hidden_dim)\n",
    "        x2 = self.enc_attention(x1, enc_outputs, enc_outputs, mask=enc_mask)\n",
    "\n",
    "        x2 = self.norm_2(x1 + self.dropout(x2))\n",
    "        x2 = self.norm_3(self.ff(x2))\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int,\n",
    "                 num_heads: int, num_layers: int, ff_dim: int, max_seq_length: int,\n",
    "                 device: torch.device, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.scaling = hidden_dim ** (0.5)\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim=hidden_dim,\n",
    "                                                  num_heads=num_heads, ff_dim=ff_dim,\n",
    "                                                  dropout_ratio=dropout_ratio)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, dec_input: Tensor, enc_outputs: Tensor,\n",
    "                enc_mask: Tensor, dec_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_inputs of shape (batch_size, seq_legth)\n",
    "            enc_outputs of shape (batch_size, seq_legth, hidden_dim)\n",
    "            dec_mask of shape (batch_size, 1, seq_length, seq_length)\n",
    "            enc_mask of shape (batch_size, 1, 1, seq_length)\n",
    "        Outputs\n",
    "            log-probabilities of shape (batch_size, seq_legth, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = dec_input.size()\n",
    "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "        x = self.token_embedding(dec_input) * self.scaling + self.positional_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # x.shape (batch_size, seq_legth, hidden_dim)\n",
    "            x = layer(x, enc_outputs, enc_mask, dec_mask)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, vocab_size)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorModel(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, enc_pad_token: int,\n",
    "                 dec_pad_token: int, device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.enc_pad_token = enc_pad_token\n",
    "        self.dec_pad_token = dec_pad_token\n",
    "        self.device = device\n",
    "\n",
    "    def _create_enc_mask(self, enc_inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            enc_inputs of shape (batch_size, seq_length)\n",
    "        Outputs\n",
    "            mask with 0s for PAD tokens of shape (batch_size, 1, 1, seq_length)\n",
    "        \"\"\"\n",
    "        return (enc_inputs != self.enc_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def _create_dec_mask(self, dec_inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            dec_inputs of shape (batch_size, seq_length)\n",
    "        Outputs\n",
    "            masks PAD tokens and future tokens; shape (batch_size, 1, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "        # mask_1 shape (batch_size, 1, 1, seq_length)\n",
    "        mask_1 = (dec_inputs != self.dec_pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        batch_size, seq_length = dec_inputs.size()\n",
    "\n",
    "        # mask_2 shape (seq_length, seq_length)\n",
    "        mask_2 = torch.tril(torch.ones((seq_length, seq_length), device= self.device)).bool()\n",
    "\n",
    "        # mask_2 shape (batch_size, 1, seq_length, seq_length)\n",
    "        mask_2 = mask_2.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "        mask = mask_1 & mask_2\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            src of shape (batch_size, src_seq_length)\n",
    "            tgt of shape (batch_size, tgt_seq_length)\n",
    "        Outputs\n",
    "            decoded sequence of shape (batch_size, seq_length, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        enc_mask = self._create_enc_mask(src)\n",
    "        dec_mask = self._create_dec_mask(tgt)\n",
    "\n",
    "        enc_outputs = self.encoder(src, enc_mask)\n",
    "        dec_outputs = self.decoder(tgt, enc_outputs, enc_mask, dec_mask)\n",
    "\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num parameters: 7,080,864\n"
     ]
    }
   ],
   "source": [
    "def init_weights(model: nn.Module) -> None:\n",
    "    if hasattr(model, \"weight\") and model.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(model.weight.data)\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "loss_function = nn.NLLLoss(ignore_index=tgt_lang.token_to_id(\"<PAD>\"))\n",
    "\n",
    "dropout_ratio = 0.3\n",
    "hidden_dim = 256\n",
    "ff_dim = 512\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "encoder = Encoder(vocab_size=src_lang.get_vocab_size(),\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  num_heads=num_heads,\n",
    "                  num_layers=num_layers,\n",
    "                  ff_dim=ff_dim,\n",
    "                  max_seq_length=100,\n",
    "                  device=device,\n",
    "                  dropout_ratio=dropout_ratio).to(device)\n",
    "\n",
    "decoder = Decoder(vocab_size=tgt_lang.get_vocab_size(),\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  num_heads=num_heads,\n",
    "                  num_layers=num_layers,\n",
    "                  ff_dim=ff_dim,\n",
    "                  max_seq_length=100,\n",
    "                  device=device,\n",
    "                  dropout_ratio=dropout_ratio).to(device)\n",
    "\n",
    "translator = TranslatorModel(encoder=encoder,\n",
    "                             decoder=decoder,\n",
    "                             enc_pad_token=src_lang.token_to_id(\"<PAD>\"),\n",
    "                             dec_pad_token=tgt_lang.token_to_id(\"<PAD>\"),\n",
    "                             device=device).to(device)\n",
    "\n",
    "translator.apply(init_weights)\n",
    "print(f\"Model num parameters: {count_parameters(translator):,}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(translator.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(model: nn.Module,\n",
    "              loss_function: nn.NLLLoss,\n",
    "              batch: dict[str, Tensor],\n",
    "              device: torch.device) -> float:\n",
    "    # src.shape (batch_size, seq_length)\n",
    "    src = batch[\"de_ids\"].to(device).transpose(1, 0)  # type: Tensor\n",
    "    tgt = batch[\"en_ids\"].to(device).transpose(1, 0)  # type: Tensor\n",
    "\n",
    "    log_probs = model(src, tgt[:, :-1])  # type: Tensor\n",
    "    log_probs = log_probs.reshape(-1, log_probs.size(-1))\n",
    "\n",
    "    loss = loss_function(log_probs, tgt[:, 1:].reshape(-1).long())\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    loss_function: nn.NLLLoss,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    device: torch.device) -> float:\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = run_batch(model=model, loss_function=loss_function, batch=batch, device=device)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(translator.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "def translate_from_tensor(model: nn.Module, src: Tensor,\n",
    "                          tgt_lang: Tokenizer, tgt: Tensor) -> str:\n",
    "    # outputs.shape (batch_size, tgt_seq_length, tgt_vocab_size)\n",
    "    log_probs = model(src, tgt=tgt)  # type: Tensor\n",
    "\n",
    "    # pred_top2.shape (batch_size, tgt_seq_length, 2)\n",
    "    _, pred_top2 = log_probs.topk(2, dim=-1)\n",
    "\n",
    "    # pred_top2.shape (tgt_seq_length, 2)\n",
    "    pred_top2 = pred_top2.squeeze(0)  # because batch_size=1 here\n",
    "\n",
    "    # unpack first 2 top predictions\n",
    "    first_pred, second_pred = pred_top2[:, 0].unsqueeze(1), pred_top2[:, 1].unsqueeze(1)\n",
    "\n",
    "    # in case first top prediction is UNK use second top prediction\n",
    "    unk_idx = tgt_lang.token_to_id(\"<UNK>\")\n",
    "    indices = torch.where(first_pred == unk_idx, second_pred, first_pred)\n",
    "\n",
    "    indices = indices.squeeze().tolist()\n",
    "\n",
    "    sentence = tgt_lang.decode(indices, skip_special_tokens=False)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def translate(model: TranslatorModel, src: Tensor | str, src_lang: Tokenizer,\n",
    "              tgt_lang: Tokenizer, max_tgt_length: int, device: torch.device,\n",
    "              clean: bool = False) -> str:\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(src, str):\n",
    "        nlp = spacy.load(\"de_core_news_sm\")  # TODO: don't like this workaround\n",
    "        tokens = [token.text.lower() for token in nlp(src)]\n",
    "        tokens = [\"<BOS>\", *tokens, \"<EOS>\"]\n",
    "        src_idxs = np.array([src_lang.token_to_id(word) for word in tokens])\n",
    "        src = torch.from_numpy(src_idxs).reshape(1, -1).to(device)  # (batch_size, src_seq_length)\n",
    "\n",
    "    enc_mask = model._create_enc_mask(src)  # noqa: SLF001\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(src, enc_mask)\n",
    "\n",
    "    tgt_indices = [tgt_lang.token_to_id(\"<BOS>\")]\n",
    "\n",
    "    for _ in range(max_tgt_length):\n",
    "        tgt = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "        dec_mask = model._create_dec_mask(tgt)  # noqa: SLF001\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_probs = model.decoder(tgt, encoder_outputs, enc_mask, dec_mask)  # type: Tensor\n",
    "\n",
    "        # pred_top2.shape (batch_size, tgt_seq_length, 2)\n",
    "        _, pred_top2 = log_probs.topk(2, dim=-1)\n",
    "\n",
    "        # pred_top2.shape (tgt_seq_length, 2)\n",
    "        pred_top2 = pred_top2.squeeze(0)  # because batch_size=1 here\n",
    "\n",
    "        # unpack first 2 top predictions\n",
    "        first_pred, second_pred = pred_top2[-1, 0].item(), pred_top2[-1, 1].item()\n",
    "\n",
    "        # in case first top prediction is UNK use second top prediction\n",
    "        unk_idx = tgt_lang.token_to_id(\"<UNK>\")\n",
    "        pred_token = second_pred if first_pred == unk_idx else first_pred\n",
    "\n",
    "        tgt_indices.append(pred_token)\n",
    "\n",
    "        if pred_token == tgt_lang.token_to_id(\"<EOS>\"):\n",
    "            break\n",
    "\n",
    "    sentence = tgt_lang.decode(tgt_indices, skip_special_tokens=clean)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def print_sentences(data: datasets.Dataset, idx: int, model: nn.Module,\n",
    "                    src_lang: Tokenizer, tgt_lang: Tokenizer, device: torch.device) -> None:\n",
    "    data_eval_src = data[idx][\"de_ids\"].to(device)\n",
    "    data_eval_tgt = data[idx][\"en_ids\"].to(device)\n",
    "    sentence_src = src_lang.decode(data_eval_src.detach().cpu().numpy().squeeze())\n",
    "    sentence_tgt = tgt_lang.decode(data_eval_tgt.detach().cpu().numpy().squeeze())\n",
    "    sentence_evaluated = translate_from_tensor(model, data_eval_src.unsqueeze(0), tgt_lang,\n",
    "                                               data_eval_tgt.unsqueeze(0))\n",
    "\n",
    "    print(f\"SOURCE: {sentence_src}\")\n",
    "    print(f\"TARGET: {sentence_tgt}\")\n",
    "    print(f\"MODEL: {sentence_evaluated}\")\n",
    "\n",
    "def evaluate_model(model: nn.Module,\n",
    "                   data_loader: torch.utils.data.DataLoader,\n",
    "                   loss_function: nn.NLLLoss,\n",
    "                   device: torch.device) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(data_loader):\n",
    "            loss = run_batch(model=model, loss_function=loss_function, batch=batch, device=device)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> a man walking past a red car. <EOS>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model=translator,\n",
    "          src=train_data[42][\"de_ids\"].reshape(1, -1).to(device),\n",
    "          src_lang=src_lang,\n",
    "          tgt_lang=tgt_lang,\n",
    "          max_tgt_length=10,\n",
    "          device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: ein mann mit sonnenbrille legt seinen arm um eine frau in einer schwarz - weißen bluse.\n",
      "TARGET: a man in sunglasses puts his arm around a woman in a black and white blouse.\n",
      "MODEL: a man with sunglasses is his woman into a woman in a white shirt white shirt. <EOS>.\n"
     ]
    }
   ],
   "source": [
    "# print untrained model translations\n",
    "print_sentences(data=train_data, idx=47, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training time on RTX 4090 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [02:04<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 1, elapsed: 125 sec, train loss: 4.8214, validation loss: 3.9135\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [02:53<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 2, elapsed: 174 sec, train loss: 3.7449, validation loss: 3.4478\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [02:43<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 3, elapsed: 164 sec, train loss: 3.3629, validation loss: 3.1322\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [02:59<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 4, elapsed: 180 sec, train loss: 3.0509, validation loss: 2.7958\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [03:01<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state...\n",
      "Epoch: 5, elapsed: 182 sec, train loss: 2.7744, validation loss: 2.5852\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "translator.train()\n",
    "\n",
    "model_name = f\"translator_transformer_v2_{num_layers}_layers\"\n",
    "best_val_loss = float(\"inf\")\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    epoch_loss = train_one_epoch(translator, optimizer,\n",
    "                                 loss_function, train_data_loader, device)\n",
    "\n",
    "    time_passed_seconds = time.time() - time_start\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    valid_loss = evaluate_model(model=translator, data_loader=valid_data_loader, loss_function=loss_function, device=device)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_val_loss:\n",
    "        # save best validaiton loss model\n",
    "        best_val_loss = valid_loss\n",
    "        print(\"Saving model state...\")\n",
    "        torch.save(translator.state_dict(), f\"models/{model_name}_bestval.pt\")\n",
    "\n",
    "    # save model\n",
    "    torch.save(translator.state_dict(), f\"models/{model_name}.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, elapsed: {time_passed_seconds:.0f} sec, train loss: {epoch_loss:.4f}, validation loss: {valid_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        random_eval_idx = int(np.random.choice(list(range(len(train_data)))))\n",
    "        print_sentences(data=train_data, idx=random_eval_idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)\n",
    "\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, color=\"blue\", label=\"Train\")\n",
    "plt.plot(valid_losses, color=\"red\", label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Average loss per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.load_state_dict(torch.load(f\"models/translator_transformer_v2_{num_layers}_layers_bestval.pt\", map_location=device))\n",
    "translator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_eval_idx = int(np.random.choice(list(range(len(train_data)))))\n",
    "print_sentences(data=train_data, idx=random_eval_idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [42, 422, 10, 7, 999]\n",
    "\n",
    "for idx in idxs:\n",
    "    print_sentences(data=train_data, idx=idx, model=translator, src_lang=src_lang, tgt_lang=tgt_lang, device=device)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def get_tokenizer_fn(tgt_tokenizer: Tokenizer):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = tgt_tokenizer.encode(s).tokens\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn\n",
    "\n",
    "tokenizer_fn = get_tokenizer_fn(tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute BLEU metric on test data\n",
    "predictions, references = [], []\n",
    "for idx in tqdm(range(test_data.num_rows)):\n",
    "    data_eval_src = test_data[idx][\"de_ids\"].reshape(1, -1).to(device)\n",
    "    sentence_evaluated = translate(model=translator,\n",
    "                                   src=data_eval_src,\n",
    "                                   src_lang=src_lang,\n",
    "                                   tgt_lang=tgt_lang,\n",
    "                                   max_tgt_length=100,\n",
    "                                   device=device,\n",
    "                                   clean=True)\n",
    "\n",
    "\n",
    "    predictions.append(sentence_evaluated)\n",
    "    references.append(test_data[idx][\"en\"])\n",
    "\n",
    "bleu.compute(predictions=predictions, references=references, tokenizer=tokenizer_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
