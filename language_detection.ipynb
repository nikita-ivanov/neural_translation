{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterator\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will build a model that determines text language - either German or English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We will use the same dataset that we used for the neural translation exercise. It consists of 200k German-English sentence pairs. We will build a new dataset of the form sentence - label (EN or DE). New dataset will be of size 400k (because each example in the original dataset will correspond to 2 examples in the new dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"IWSLT/iwslt2017\", \"iwslt2017-de-en\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw, valid_data_raw, test_data_raw = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")\n",
    "\n",
    "MAX_NUM_SAMPLES = None\n",
    "if MAX_NUM_SAMPLES is not None:\n",
    "    train_data_raw = train_data_raw.take(MAX_NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer\n",
    "\n",
    "In order to be able to work with text data, we need to be able to convert it into numeric form that neural networks can process. This is, of course, done by means of a tokenizer. I chose `tiktoken` tokenizer because it is multilingual and fast, but of course other tokenizers could do the job as well. One crucial requirement to the tokenizer is that it was train on a corpora that contained English and German sentences (and possibly other languages as well, such as `tiktoken`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We first tokenize all the data in our dataset before transofrming it into the target form.\n",
    " We will only take first 20 tokens as that should be more than enough to deduce a language of a sentence.\n",
    " Number 20 was picked arbitrary, maybe even lower number will do the job equally well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example: dict[str, dict[str, str]],\n",
    "                     tokenizer: tiktoken.core.Encoding,\n",
    "                     max_length: int) -> dict[str, list[int]]:\n",
    "    de_tokens = tokenizer.encode(example[\"translation\"][\"de\"])[:max_length]\n",
    "    en_tokens = tokenizer.encode(example[\"translation\"][\"en\"])[:max_length]\n",
    "\n",
    "    return {\n",
    "        \"de_tokens\": de_tokens,\n",
    "        \"en_tokens\": en_tokens,\n",
    "    }\n",
    "\n",
    "MAX_SEQ_LENGTH = 20  # we will be using first 20 tokens to classify a sentence as German or Englsih\n",
    "fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": MAX_SEQ_LENGTH}\n",
    "\n",
    "train_data_raw = train_data_raw.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data_raw = valid_data_raw.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data_raw = test_data_raw.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(dataset: datasets.Dataset,\n",
    "                     tokenizer: tiktoken.core.Encoding,\n",
    "                     max_length: int) -> Iterator[dict]:\n",
    "    for example in dataset:\n",
    "        de_tokens = tokenizer.encode(example[\"translation\"][\"de\"])[:max_length]\n",
    "        en_tokens = tokenizer.encode(example[\"translation\"][\"en\"])[:max_length]\n",
    "\n",
    "        yield {\"tokens\": de_tokens, \"label\": 0, \"text\": example[\"translation\"][\"de\"]}\n",
    "        yield {\"tokens\": en_tokens, \"label\": 1, \"text\": example[\"translation\"][\"en\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5f5c30f1a841e383b58a743b8b03bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec46e2de6bb44550bd96b43decea6b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240020dda5d24731b18bf5bd25d9407d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_kwargs={\"tokenizer\": tokenizer, \"max_length\": MAX_SEQ_LENGTH}\n",
    "\n",
    "train_data = datasets.Dataset.from_generator(generate_dataset,\n",
    "                                             gen_kwargs=gen_kwargs | {\"dataset\": train_data_raw})\n",
    "\n",
    "valid_data = datasets.Dataset.from_generator(generate_dataset,\n",
    "                                             gen_kwargs=gen_kwargs | {\"dataset\": valid_data_raw})\n",
    "\n",
    "test_data = datasets.Dataset.from_generator(generate_dataset,\n",
    "                                            gen_kwargs=gen_kwargs | {\"dataset\": test_data_raw})\n",
    "\n",
    "train_data = train_data.with_format(type=\"torch\",\n",
    "                                    columns=[\"tokens\", \"label\"],\n",
    "                                    output_all_columns=True)\n",
    "\n",
    "valid_data = valid_data.with_format(type=\"torch\",\n",
    "                                    columns=[\"tokens\", \"label\"],\n",
    "                                    output_all_columns=True)\n",
    "\n",
    "test_data = test_data.with_format(type=\"torch\",\n",
    "                                  columns=[\"tokens\", \"label\"],\n",
    "                                  output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: dict) -> dict[str, int]:\n",
    "    token_ids = [example[\"tokens\"] for example in batch]\n",
    "    labels = [example[\"label\"] for example in batch]\n",
    "\n",
    "    # pad value 2 corresponds to symbol '#' in tiktoken\n",
    "    # shape (max_length, batch_size)\n",
    "    token_ids= torch.nn.utils.rnn.pad_sequence(token_ids, padding_value=2)\n",
    "\n",
    "    # shape (batch_size, max_length)\n",
    "    token_ids = token_ids.transpose(1, 0)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    labels = Tensor(labels).unsqueeze(1)\n",
    "\n",
    "    return {\"tokens\": token_ids, \"label\": labels}\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                                batch_size=64,\n",
    "                                                shuffle=True,\n",
    "                                                collate_fn=collate_fn)\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(dataset=valid_data,\n",
    "                                                batch_size=64,\n",
    "                                                shuffle=True,\n",
    "                                                collate_fn=collate_fn)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                               batch_size=64,\n",
    "                                               shuffle=True,\n",
    "                                               collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will use a simple feed forward neural network for this task. We will have 3 layers:\n",
    "- embedding layer to convert from token ids to a state space representation\n",
    "- feed forward layer 1\n",
    "- feed forward layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc_1 = torch.nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc_2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            x of shape (batch_size, seq_length)\n",
    "        Outputs\n",
    "            Tensor of shape (batch_size, 1) representing probabilities\n",
    "        \"\"\"\n",
    "        # x.shape (batch_size, seq_length, embed_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x.shape (batch_size, seq_length, hidden_dim)\n",
    "        x = self.fc_1(x)\n",
    "        x = x.relu()\n",
    "\n",
    "        # x.shape (batch_size, hidden_dim)\n",
    "        x = x.sum(dim=1)\n",
    "\n",
    "        # x.shape (batch_size, 1)\n",
    "        x = self.fc_2(x)\n",
    "        x = x.sigmoid()\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "def run_batch(model: torch.nn.Module,\n",
    "              loss_function: torch.nn.BCELoss,\n",
    "              batch: dict[str, Tensor],\n",
    "              device: torch.device) -> float:\n",
    "    # src.shape (batch_size, seq_length)\n",
    "    src = batch[\"tokens\"].to(device)  # type: Tensor\n",
    "    tgt = batch[\"label\"].to(device)  # type: Tensor\n",
    "\n",
    "    probs = model(src)  # type: Tensor\n",
    "\n",
    "    loss = loss_function(probs, tgt)  # type: Tensor\n",
    "\n",
    "    pred = torch.where(probs > 0.5, 1.0, 0.0)\n",
    "    accuracy = (pred == tgt).float().mean()\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    loss_function: torch.nn.BCELoss,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    device: torch.device) -> float:\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct = 0\n",
    "\n",
    "    progress_bar = tqdm(data_loader)\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, accuracy = run_batch(model=model, loss_function=loss_function, batch=batch, device=device)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        correct += accuracy\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            running_loss = sum(losses) / len(losses)\n",
    "            progress_bar.set_description(f\"Running loss after {i} batches: {running_loss:.2f}, accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "def evaluate_model(model: torch.nn.Module,\n",
    "                    loss_function: torch.nn.BCELoss,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    device: torch.device) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    loss, accuracy = 0, 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        loss_batch, accuracy_batch = run_batch(model=model,\n",
    "                                               loss_function=loss_function,\n",
    "                                               batch=batch,\n",
    "                                               device=device)\n",
    "\n",
    "        loss += loss_batch\n",
    "        accuracy += accuracy_batch\n",
    "\n",
    "    return loss / len(data_loader), accuracy / len(data_loader)\n",
    "\n",
    "def classify_sentence(model: Model,\n",
    "                      sentence: str,\n",
    "                      tokenizer: tiktoken.core.Encoding,\n",
    "                      max_length: int,\n",
    "                      device: torch.device) -> None:\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer.encode(sentence)[:max_length]\n",
    "\n",
    "    tokens = np.asarray(tokens)\n",
    "    tokens = torch.from_numpy(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    pred = model(tokens)\n",
    "\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 12,901,889 parameters\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model = Model(embed_dim=128, hidden_dim=512, vocab_size=tokenizer.max_token_value).to(device)\n",
    "\n",
    "def count_parameters(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18996331095695496"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check untrained model prediction\n",
    "# the value represents probability of an english sentence\n",
    "classify_sentence(model=model,\n",
    "                  sentence=\"hello world, how is it going?\",\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_length=MAX_SEQ_LENGTH,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loss after 6400 batches: 0.02, accuracy: 100.00%: 100%|██████████| 6441/6441 [02:21<00:00, 45.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1 - validation loss 0.00, validation accuracy 1.00\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 1\n",
    "losses = []\n",
    "\n",
    "# training on M2 chip takes around 2 minutes 20 seconds (1 epoch)\n",
    "for epoch in range(num_epochs):\n",
    "    loss_epoch = train_one_epoch(model=model,\n",
    "                                 optimizer=optimizer,\n",
    "                                 loss_function=loss_function,\n",
    "                                 data_loader=train_data_loader,\n",
    "                                 device=device)\n",
    "\n",
    "    valid_loss, valid_accuracy = evaluate_model(model=model,\n",
    "                                                loss_function=loss_function,\n",
    "                                                data_loader=valid_data_loader,\n",
    "                                                device=device)\n",
    "\n",
    "    losses.append(loss_epoch)\n",
    "\n",
    "    print(f\"Finished epoch {epoch + 1} - validation loss {valid_loss:.2f}, validation accuracy {valid_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.01, test accuracy 1.00\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model=model,\n",
    "                                          loss_function=loss_function,\n",
    "                                          data_loader=test_data_loader,\n",
    "                                          device=device)\n",
    "\n",
    "print(f\"Test loss {test_loss:.2f}, test accuracy {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3911486007600615e-07"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_sentence(model=model,\n",
    "                  sentence=\"hallo welt, wie gehts es?\",\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_length=MAX_SEQ_LENGTH,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_sentence(model=model,\n",
    "                  sentence=\"hello world, how is it going?\",\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_length=MAX_SEQ_LENGTH,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7549492716789246"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_sentence(model=model,\n",
    "                  sentence=\"hello world\",\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_length=MAX_SEQ_LENGTH,\n",
    "                  device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmetal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
